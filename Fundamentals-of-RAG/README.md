# RAG Demos - Retrieval-Augmented Generation Examples

# RAGæ¼”ç¤º â€”â€” æ£€ç´¢å¢å¼ºç”Ÿæˆç¤ºä¾‹

A comprehensive collection of Python utilities and demonstrations for building Retrieval-Augmented Generation (RAG) systems. This repository contains parsers, chunkers, vector databases, and complete RAG implementations to help you understand and build your own RAG applications.

ä¸€ä¸ªå…¨é¢çš„Pythonå·¥å…·å’Œæ¼”ç¤ºé›†åˆï¼Œç”¨äºæ„å»ºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿã€‚è¿™ä¸ªä»“åº“åŒ…å«è§£æå™¨ã€åˆ†å—å™¨ã€å‘é‡æ•°æ®åº“ä»¥åŠå®Œæ•´çš„RAGå®ç°ï¼Œå¸®åŠ©ä½ ç†è§£å¹¶æ„å»ºè‡ªå·±çš„RAGåº”ç”¨ç¨‹åºã€‚

---

## ğŸ¯ Overview / æ¦‚è§ˆ

This repository provides practical examples and reusable tools for every stage of the RAG pipeline:

æœ¬ä»“åº“ä¸ºRAGæµç¨‹çš„æ¯ä¸ªé˜¶æ®µæä¾›äº†å®ç”¨çš„ç¤ºä¾‹å’Œå¯é‡å¤ä½¿ç”¨çš„å·¥å…·ï¼š

* **Document Parsing / æ–‡æ¡£è§£æ** : Extract text from PDFs, Word docs, CSV, and plain text (ä»PDFã€Wordæ–‡æ¡£ã€CSVå’Œçº¯æ–‡æœ¬ä¸­æå–æ–‡æœ¬)
* **Text Chunking / æ–‡æœ¬åˆ†å—** : Multiple strategies for splitting documents intelligently (æ™ºèƒ½åˆ†å‰²æ–‡æ¡£çš„å¤šç§ç­–ç•¥)
* **Vector Storage / å‘é‡å­˜å‚¨** : Examples with ChromaDB and FAISS (ä½¿ç”¨ChromaDBå’ŒFAISSçš„ç¤ºä¾‹)
* **Semantic Search / è¯­ä¹‰æœç´¢** : BM25, vector search, and hybrid approaches (BM25å…³é”®å­—æœç´¢ã€å‘é‡æœç´¢ä»¥åŠæ··åˆæœç´¢æ–¹æ³•)
* **Complete RAG / å®Œæ•´çš„RAG** : End-to-end implementations with Ollama and LLMs (åŸºäºOllamaå’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„ç«¯åˆ°ç«¯å®ç°)

---

## ğŸ“ Repository Structure / ä»“åº“ç»“æ„

### ğŸ” Complete RAG Systems / å®Œæ•´çš„RAGç³»ç»Ÿ

#### **BookSearch/**

Full-featured RAG implementations with progressive complexity.

åŠŸèƒ½é½å…¨çš„RAGå®ç°ï¼Œå¤æ‚åº¦é€æ­¥é€’å¢ã€‚

**Contains / åŒ…å«:**

* `app_v1.py` - Basic RAG demo with in-memory documents (åŸºç¡€RAGæ¼”ç¤ºï¼Œä½¿ç”¨å†…å­˜æ–‡æ¡£)
* `app_v2.py` - File-based RAG with ingestion pipeline (åŸºäºæ–‡ä»¶çš„RAGï¼ŒåŒ…å«æ•°æ®æ‘„å…¥ç®¡é“)
* `hybrid_rag.py` - Hybrid search (BM25 + semantic vector search with RRF) (æ··åˆæœç´¢ï¼šBM25 + åŸºäºå€’æ•°æ’åèåˆRRFçš„è¯­ä¹‰å‘é‡æœç´¢)

**Key Features / æ ¸å¿ƒç‰¹æ€§:**

* Ollama integration (llama3.3 + nomic-embed-text) (Ollamaé›†æˆ)
* ChromaDB vector storage (ChromaDBå‘é‡å­˜å‚¨)
* Automatic document chunking with overlap (å¸¦æœ‰é‡å éƒ¨åˆ†çš„è‡ªåŠ¨æ–‡æ¡£åˆ†å—)
* Source citation in answers (å›ç­”ä¸­åŒ…å«æ¥æºå¼•ç”¨)
* Reciprocal Rank Fusion (RRF) for hybrid search (ç”¨äºæ··åˆæœç´¢çš„å€’æ•°æ’åèåˆæŠ€æœ¯)

**Use Case / åº”ç”¨åœºæ™¯:** Question-answering over book collections (Shakespeare, Sherlock Holmes, Frankenstein, etc.) (é’ˆå¯¹ä¹¦ç±é›†åˆçš„é—®ç­”ç³»ç»Ÿï¼Œå¦‚èå£«æ¯”äºšã€ç¦å°”æ‘©æ–¯ç­‰)

[ğŸ“– Full Documentation / å®Œæ•´æ–‡æ¡£](https://www.google.com/search?q=./BookSearch/README.md)

---

### ğŸ’¾ Vector Database Demos / å‘é‡æ•°æ®åº“æ¼”ç¤º

#### **ChromaDemo/**

Minimal ChromaDB implementation for semantic search.

ç”¨äºè¯­ä¹‰æœç´¢çš„æç®€ChromaDBå®ç°ã€‚

**Contains / åŒ…å«:**

* `ingest_and_query.py` - Ingest text files and perform semantic queries (æ‘„å…¥æ–‡æœ¬æ–‡ä»¶å¹¶æ‰§è¡Œè¯­ä¹‰æŸ¥è¯¢)

**Key Features / æ ¸å¿ƒç‰¹æ€§:**

* Persistent ChromaDB storage (æŒä¹…åŒ–çš„ChromaDBå­˜å‚¨)
* Automatic chunking (1500 chars, 200 overlap) (è‡ªåŠ¨åˆ†å—ï¼š1500å­—ç¬¦ï¼Œ200å­—ç¬¦é‡å )
* Sentence-transformers embeddings (all-MiniLM-L6-v2) (Sentence-transformersåµŒå…¥æ¨¡å‹)
* Metadata filtering (å…ƒæ•°æ®è¿‡æ»¤)
* Idempotent ingestion (å¹‚ç­‰æ•°æ®æ‘„å…¥ï¼Œé˜²æ­¢é‡å¤)

**Use Case / åº”ç”¨åœºæ™¯:** Simple semantic search over text document collections (é’ˆå¯¹æ–‡æœ¬æ–‡æ¡£é›†åˆçš„ç®€å•è¯­ä¹‰æœç´¢)

[ğŸ“– Full Documentation / å®Œæ•´æ–‡æ¡£](https://www.google.com/search?q=./ChromaDemo/README.md)

---

### ğŸ“„ Document Parsers / æ–‡æ¡£è§£æå™¨

#### **PDFParser/**

Extract and chunk text from PDF documents for RAG ingestion.

ä»PDFæ–‡æ¡£ä¸­æå–å¹¶åˆ†å—æ–‡æœ¬ï¼Œç”¨äºRAGæ•°æ®æ‘„å…¥ã€‚

**Contains / åŒ…å«:**

* `pdf_parser.py` - PDFParser class implementation (PDFParserç±»å®ç°)
* `main.py` - Usage examples (ä½¿ç”¨ç¤ºä¾‹)

**Key Features / æ ¸å¿ƒç‰¹æ€§:**

* PyPDF2-based text extraction (åŸºäºPyPDF2çš„æ–‡æœ¬æå–)
* LangChain text splitter integration (LangChainæ–‡æœ¬åˆ†å‰²å™¨é›†æˆ)
* Intelligent boundary detection (æ™ºèƒ½è¾¹ç•Œæ£€æµ‹)
* Structured JSON output with metadata (å¸¦æœ‰å…ƒæ•°æ®çš„ç»“æ„åŒ–JSONè¾“å‡º)
* Multiple output formats (JSON, text) (å¤šç§è¾“å‡ºæ ¼å¼ï¼šJSONã€çº¯æ–‡æœ¬)

**Dependencies / ä¾èµ–é¡¹:** PyPDF2, langchain-text-splitters

[ğŸ“– Full Documentation / å®Œæ•´æ–‡æ¡£](https://www.google.com/search?q=./PDFParser/README.md)

---

#### **WordParser/**

Parse Microsoft Word (.docx) documents for RAG systems.

ä¸ºRAGç³»ç»Ÿè§£æMicrosoft Word (.docx) æ–‡æ¡£ã€‚

**Contains / åŒ…å«:**

* `main.py` - DocxParser class and demo (DocxParserç±»åŠæ¼”ç¤º)

**Key Features / æ ¸å¿ƒç‰¹æ€§:**

* Extracts text from .docx files (ä».docxæ–‡ä»¶æå–æ–‡æœ¬)
* Document properties (title, author, dates) (æå–æ–‡æ¡£å±æ€§ï¼šæ ‡é¢˜ã€ä½œè€…ã€æ—¥æœŸç­‰)
* Hierarchical chunking (paragraph â†’ sentence â†’ word) (å±‚çº§åˆ†å—ï¼šæ®µè½ â†’ å¥å­ â†’ å•è¯)
* Preserves paragraph structure (ä¿ç•™æ®µè½ç»“æ„)
* MD5-based document IDs (åŸºäºMD5çš„æ–‡æ¡£ID)

**Dependencies / ä¾èµ–é¡¹:** python-docx, lxml

[ğŸ“– Full Documentation / å®Œæ•´æ–‡æ¡£](https://www.google.com/search?q=./WordParser/README.md)

---

#### **TextParser/**

Parse plain text files with smart chunking.

ä½¿ç”¨æ™ºèƒ½åˆ†å—æŠ€æœ¯è§£æçº¯æ–‡æœ¬æ–‡ä»¶ã€‚

**Contains / åŒ…å«:**

* `main.py` - TextDocumentParser class and demo (TextDocumentParserç±»åŠæ¼”ç¤º)

**Key Features / æ ¸å¿ƒç‰¹æ€§:**

* Zero dependencies (Python stdlib only) (é›¶å¤–éƒ¨ä¾èµ–ï¼Œä»…éœ€Pythonæ ‡å‡†åº“)
* Sentence-aware boundary detection (æ„ŸçŸ¥å¥å­çš„è¾¹ç•Œæ£€æµ‹)
* Rich file metadata extraction (ä¸°å¯Œçš„çš„æ–‡ä»¶å…ƒæ•°æ®æå–)
* Configurable chunk size and overlap (å¯é…ç½®çš„åˆ†å—å¤§å°å’Œé‡å é‡)
* MD5 document IDs (MD5æ–‡æ¡£ID)

**Use Case / åº”ç”¨åœºæ™¯:** Processing plain text documents, logs, markdown files (å¤„ç†çº¯æ–‡æœ¬æ–‡æ¡£ã€æ—¥å¿—ã€Markdownæ–‡ä»¶)

[ğŸ“– Full Documentation / å®Œæ•´æ–‡æ¡£](https://www.google.com/search?q=./TextParser/README.md)

---

#### **CSVParser/**

Convert CSV data to RAG-ready JSON documents.

å°†CSVæ•°æ®è½¬æ¢ä¸ºæ”¯æŒRAGçš„JSONæ–‡æ¡£ã€‚

**Contains / åŒ…å«:**

* `main.py` - CSV to JSON converter (CSVè½¬JSONè½¬æ¢å™¨)
* Sample datasets (sample_data.csv, big_data.csv) (ç¤ºä¾‹æ•°æ®é›†)

**Key Features / æ ¸å¿ƒç‰¹æ€§:**

* Transforms rows into searchable text (å°†æ•°æ®è¡Œè½¬æ¢ä¸ºå¯æœç´¢æ–‡æœ¬)
* Preserves all columns as metadata (å°†æ‰€æœ‰åˆ—ä¿ç•™ä¸ºå…ƒæ•°æ®)
* JSON output for vector database ingestion (è¾“å‡ºJSONä»¥ä¾›å‘é‡æ•°æ®åº“æ‘„å…¥)
* No external dependencies (æ— å¤–éƒ¨ä¾èµ–)

**Use Case / åº”ç”¨åœºæ™¯:** Making structured data semantically searchable (ä½¿ç»“æ„åŒ–æ•°æ®å…·å¤‡è¯­ä¹‰æœç´¢èƒ½åŠ›)

[ğŸ“– Full Documentation / å®Œæ•´æ–‡æ¡£](https://www.google.com/search?q=./CSVParser/README.md)

---

### âœ‚ï¸ Text Chunking / æ–‡æœ¬åˆ†å—

#### **ChunkingDemo/**

Comprehensive document chunker with 8 different strategies.

åŒ…å«8ç§ä¸åŒç­–ç•¥çš„ç»¼åˆæ–‡æ¡£åˆ†å—å™¨ã€‚

**Contains / åŒ…å«:**

* `document_chunker.py` - CLI tool with multiple chunking methods (æ”¯æŒå¤šç§åˆ†å—æ–¹æ³•çš„å‘½ä»¤è¡Œå·¥å…·)

**Chunking Methods / åˆ†å—æ–¹æ³•:**

1. **Line-by-Line / é€è¡Œ** : Group by number of lines (æŒ‰è¡Œæ•°åˆ†ç»„)
2. **Fixed Size / å›ºå®šå¤§å°** : Fixed character chunks with overlap (å›ºå®šå­—ç¬¦åˆ†å—ï¼Œå¸¦é‡å )
3. **Sliding Window / æ»‘åŠ¨çª—å£** : Overlapping windows (é‡å çª—å£)
4. **Sentence-Based / åŸºäºå¥å­** : Group by sentences (æŒ‰å¥å­åˆ†ç»„)
5. **Paragraph-Based / åŸºäºæ®µè½** : Group by paragraphs (æŒ‰æ®µè½åˆ†ç»„)
6. **Page-Based / åŸºäºé¡µé¢** : Simulate pages (by line count) (æ¨¡æ‹Ÿé¡µé¢ï¼Œé€šè¿‡è¡Œæ•°)
7. **Section-Based / åŸºäºç« èŠ‚** : Split on headings (markdown, etc.) (æŒ‰æ ‡é¢˜åˆ†å‰²ï¼Œå¦‚Markdown)
8. **Token-Based / åŸºäºToken** : BERT tokenizer-based chunking (åŸºäºBERTåˆ†è¯å™¨çš„åˆ†å—)

**Supports / æ”¯æŒæ ¼å¼:** TXT, PDF, DOC, DOCX files

**Dependencies / ä¾èµ–é¡¹:** PyPDF2, python-docx, transformers (optional/å¯é€‰)

[ğŸ“– Full Documentation / å®Œæ•´æ–‡æ¡£](https://www.google.com/search?q=./ChunkingDemo/README.md)

---

### ğŸ” Search Examples / æœç´¢ç¤ºä¾‹

#### **SearchTool/**

Jupyter notebooks demonstrating different search approaches.

æ¼”ç¤ºä¸åŒæœç´¢æ–¹æ³•çš„Jupyter Notebooksã€‚

**Contains / åŒ…å«:**

* `BM25-vs-Semantic.ipynb` - Comparison of keyword vs vector search (å…³é”®å­—æœç´¢ä¸å‘é‡æœç´¢çš„å¯¹æ¯”)
* `SearchDemo.ipynb` - Search implementation examples (æœç´¢å®ç°ç¤ºä¾‹)
* `Semantic-Demo.ipynb` - Semantic search demonstration (è¯­ä¹‰æœç´¢æ¼”ç¤º)

**Key Topics / æ ¸å¿ƒä¸»é¢˜:**

* BM25 keyword search (BM25å…³é”®å­—æœç´¢)
* Vector embeddings and similarity (å‘é‡åµŒå…¥ä¸ç›¸ä¼¼åº¦)
* Hybrid search strategies (æ··åˆæœç´¢ç­–ç•¥)
* Search quality comparison (æœç´¢è´¨é‡å¯¹æ¯”)

**Use Case / åº”ç”¨åœºæ™¯:** Understanding search approaches for RAG (ç†è§£RAGçš„å„ç§æœç´¢æ–¹æ³•)

---

#### **SearchFiles/**

Collection of classic literature texts for search and RAG testing.

ç”¨äºæœç´¢å’ŒRAGæµ‹è¯•çš„ç»å…¸æ–‡å­¦æ–‡æœ¬é›†åˆã€‚

**Contains / åŒ…å«:** 18 classic books in plain text format (18æœ¬çº¯æ–‡æœ¬æ ¼å¼çš„ç»å…¸ä¹¦ç±ï¼ŒåŒ…æ‹¬ã€Šå“ˆå…‹è´åˆ©Â·è´¹æ©å†é™©è®°ã€‹ã€ã€Šç¦å°”æ‘©æ–¯æ¢æ¡ˆé›†ã€‹ã€ã€Šçˆ±ä¸½ä¸æ¢¦æ¸¸ä»™å¢ƒã€‹ã€ã€Šç§‘å­¦æ€ªäººã€‹ç­‰)

**Use Case / åº”ç”¨åœºæ™¯:** Test corpus for RAG and search implementations (ä½œä¸ºRAGå’Œæœç´¢å®ç°çš„æµ‹è¯•è¯­æ–™åº“)

---

## ğŸš€ Quick Start / å¿«é€Ÿå¼€å§‹

### Prerequisites / å‰ç½®è¦æ±‚

* Python 3.8 or higher (Python 3.8åŠä»¥ä¸Šç‰ˆæœ¬)
* pip package manager (pipåŒ…ç®¡ç†å™¨)

### Basic Installation / åŸºç¡€å®‰è£…

Most demos have their own `requirements.txt`. Install per project:

å¤§å¤šæ•°æ¼”ç¤ºç›®å½•éƒ½æœ‰å„è‡ªçš„ `requirements.txt`ã€‚è¯·é’ˆå¯¹æ¯ä¸ªé¡¹ç›®å•ç‹¬å®‰è£…ï¼š

**Bash**

```
cd BookSearch
pip install -r requirements.txt
```

### Quick Examples / å¿«é€Ÿç¤ºä¾‹

**1. Basic RAG with BookSearch (BookSearchåŸºç¡€RAG):**

**Bash**

```
cd BookSearch
python app_v2.py init          # Check setup (æ£€æŸ¥è®¾ç½®)
python app_v2.py ingest        # Ingest documents (æ‘„å…¥æ–‡æ¡£)
python app_v2.py ask "Who is Sherlock Holmes?" # (æé—®)
```

**2. ChromaDB Semantic Search (ChromaDBè¯­ä¹‰æœç´¢):**

**Bash**

```
cd ChromaDemo
pip install -r requirements.txt
python ingest_and_query.py
```

**3. Parse a PDF (è§£æPDF):**

**Bash**

```
cd PDFParser
pip install -r requirements.txt
python main.py
```

**4. Parse a Word Document (è§£æWordæ–‡æ¡£):**

**Bash**

```
cd WordParser
pip install -r requirements.txt
python main.py
```

**5. Chunk a Document (æ–‡æ¡£åˆ†å—):**

**Bash**

```
cd ChunkingDemo
pip install -r requirements.txt
python document_chunker.py sample_document.txt sentence --max-sentences 5
```

**6. Convert CSV to RAG Format (å°†CSVè½¬æ¢ä¸ºRAGæ ¼å¼):**

**Bash**

```
cd CSVParser
python main.py
```

---

## ğŸ—ï¸ Building Your Own RAG System / æ„å»ºä½ è‡ªå·±çš„RAGç³»ç»Ÿ

### Step-by-Step Approach / åˆ†æ­¥æŒ‡å—

1. **Choose Your Parser / é€‰æ‹©è§£æå™¨** (based on document format / åŸºäºæ–‡æ¡£æ ¼å¼)
   * PDF â†’ PDFParser
   * Word â†’ WordParser
   * Plain text (çº¯æ–‡æœ¬) â†’ TextParser
   * Structured data (ç»“æ„åŒ–æ•°æ®) â†’ CSVParser
2. **Select Chunking Strategy / é€‰æ‹©åˆ†å—ç­–ç•¥**
   * Small chunks (300-500 chars / å­—ç¬¦) â†’ Precise retrieval (ç²¾ç¡®æ£€ç´¢)
   * Medium chunks (1000-1500 chars / å­—ç¬¦) â†’ Balanced (å¹³è¡¡)
   * Large chunks (2000+ chars / å­—ç¬¦) â†’ Maximum context (æœ€å¤§ä¸Šä¸‹æ–‡)
   * Use ChunkingDemo to experiment (ä½¿ç”¨ChunkingDemoè¿›è¡Œå®éªŒ)
3. **Pick Vector Database / æŒ‘é€‰å‘é‡æ•°æ®åº“**
   * ChromaDB â†’ Easy setup, great for prototypes (æ˜“äºè®¾ç½®ï¼Œé€‚åˆåŸå‹å¼€å‘)
   * FAISS â†’ High performance, production-ready (é«˜æ€§èƒ½ï¼Œç”Ÿäº§çº§åˆ«)
   * Pinecone/Weaviate â†’ Managed, scalable (æ‰˜ç®¡æœåŠ¡ï¼Œæ˜“äºæ‰©å±•)
4. **Implement Search / å®ç°æœç´¢**
   * Semantic only (ä»…è¯­ä¹‰) â†’ Simple, fast (ç®€å•ã€å¿«é€Ÿ)
   * Hybrid (BM25 + Vector / æ··åˆ) â†’ Best quality (æœ€ä½³è´¨é‡)
   * See SearchTool notebooks for comparisons (å‚è€ƒSearchToolä¸­çš„Notebookå¯¹æ¯”)
5. **Add LLM Generation / æ·»åŠ å¤§æ¨¡å‹ç”Ÿæˆ**
   * Ollama â†’ Local, free (æœ¬åœ°ã€å…è´¹)
   * OpenAI â†’ High quality (é«˜è´¨é‡)
   * Anthropic â†’ Long context (é•¿ä¸Šä¸‹æ–‡)

### Architecture Pattern / æ¶æ„æ¨¡å¼

**Python**

```
# 1. Parse documents / è§£ææ–‡æ¡£
from pdfparser.main import PDFParser
parser = PDFParser(chunk_size=1000, chunk_overlap=200)
chunks = parser.process_document('document.pdf')

# 2. Store in vector DB / å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
import chromadb
client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_or_create_collection("docs")
collection.add(
    ids=[f"chunk_{c['chunk_id']}" for c in chunks],
    documents=[c['text'] for c in chunks],
    metadatas=[c['document_metadata'] for c in chunks]
)

# 3. Query / æŸ¥è¯¢
results = collection.query(
    query_texts=["What is the main topic?"],
    n_results=5
)

# 4. Generate answer with LLM / ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”
import ollama
context = "\n\n".join(results['documents'][0])
prompt = f"Answer based on context:\n{context}\n\nQuestion: What is the main topic?"
answer = ollama.generate(model="llama3.3", prompt=prompt)
print(answer['response'])
```

---

## ğŸ”§ Common Configurations / å¸¸è§é…ç½®

### Chunk Size Guidelines / åˆ†å—å¤§å°æŒ‡å—

| **Document Type (æ–‡æ¡£ç±»å‹)** | **Chunk Size (åˆ†å—å¤§å°)** | **Overlap (é‡å éƒ¨åˆ†)** | **Rationale (ç†ç”±)**                  |
| ---------------------------------- | ------------------------------- | ---------------------------- | ------------------------------------------- |
| Technical docs (æŠ€æœ¯æ–‡æ¡£)          | 800-1200                        | 150-250                      | Balance detail & context (å¹³è¡¡ç»†èŠ‚ä¸ä¸Šä¸‹æ–‡) |
| Books/Articles (ä¹¦ç±/æ–‡ç« )         | 1000-1500                       | 200-300                      | Preserve narrative flow (ä¿ç•™å™è¿°è¿è´¯æ€§)    |
| Code documentation (ä»£ç æ–‡æ¡£)      | 500-800                         | 100-150                      | Precise code examples (ç²¾ç¡®çš„ä»£ç ç¤ºä¾‹)      |
| Chat logs (èŠå¤©è®°å½•)               | 300-500                         | 50-100                       | Short exchanges (çŸ­ä¿ƒçš„äº¤æµ)                |
| Research papers (ç ”ç©¶è®ºæ–‡)         | 1500-2000                       | 300-400                      | Complex arguments (å¤æ‚çš„è®ºè¯è¿‡ç¨‹)          |

### Embedding Models / åµŒå…¥æ¨¡å‹

| **Model (æ¨¡å‹)** | **Dimensions (ç»´åº¦)** | **Speed (é€Ÿåº¦)** | **Best For (æœ€é€‚ç”¨åœºæ™¯)** |
| ---------------------- | --------------------------- | ---------------------- | ------------------------------- |
| all-MiniLM-L6-v2       | 384                         | Very Fast (æå¿«)       | General purpose (é€šç”¨åœºæ™¯)      |
| nomic-embed-text       | 768                         | Fast (å¿«)              | Ollama integration (Ollamaé›†æˆ) |
| text-embedding-ada-002 | 1536                        | Medium (ä¸­ç­‰)          | High quality (OpenAI) (é«˜è´¨é‡)  |
| instructor-large       | 768                         | Slow (æ…¢)              | Domain-specific (ç‰¹å®šé¢†åŸŸ)      |

---

## ğŸ“Š Comparison Matrix / å¯¹æ¯”çŸ©é˜µ

| **Feature (ç‰¹æ€§)**     | **BookSearch** | **ChromaDemo** | **PDFParser** | **WordParser** | **TextParser** | **CSVParser** |
| ---------------------------- | -------------------- | -------------------- | ------------------- | -------------------- | -------------------- | ------------------- |
| Complete RAG (å®Œæ•´RAG)       | âœ…                   | âŒ                   | âŒ                  | âŒ                   | âŒ                   | âŒ                  |
| Vector DB (å‘é‡æ•°æ®åº“)       | âœ…                   | âœ…                   | âŒ                  | âŒ                   | âŒ                   | âŒ                  |
| LLM Integration (å¤§æ¨¡å‹é›†æˆ) | âœ…                   | âŒ                   | âŒ                  | âŒ                   | âŒ                   | âŒ                  |
| Hybrid Search (æ··åˆæœç´¢)     | âœ…                   | âŒ                   | âŒ                  | âŒ                   | âŒ                   | âŒ                  |
| PDF Support (æ”¯æŒPDF)        | âŒ                   | âŒ                   | âœ…                  | âŒ                   | âŒ                   | âŒ                  |
| Word Support (æ”¯æŒWord)      | âŒ                   | âŒ                   | âŒ                  | âœ…                   | âŒ                   | âŒ                  |
| Structured Data (ç»“æ„åŒ–æ•°æ®) | âŒ                   | âŒ                   | âŒ                  | âŒ                   | âŒ                   | âœ…                  |
| Zero Dependencies (é›¶ä¾èµ–)   | âŒ                   | âŒ                   | âŒ                  | âŒ                   | âœ…                   | âœ…                  |

---

## ğŸ› ï¸ Technology Stack / æŠ€æœ¯æ ˆ

### Core Technologies / æ ¸å¿ƒæŠ€æœ¯

* **Python 3.8+** : Primary language (ä¸»è¦ç¼–ç¨‹è¯­è¨€)
* **ChromaDB** : Vector database (å‘é‡æ•°æ®åº“)
* **Ollama** : Local LLM inference (æœ¬åœ°å¤§æ¨¡å‹æ¨ç†)
* **PyPDF2** : PDF parsing (PDFè§£æ)
* **python-docx** : Word document parsing (Wordæ–‡æ¡£è§£æ)
* **LangChain** : Text splitting utilities (æ–‡æœ¬åˆ†å‰²å·¥å…·)

### Optional Dependencies / å¯é€‰ä¾èµ–

* **rank-bm25** : BM25 search algorithm (BM25æœç´¢ç®—æ³•)
* **transformers** : BERT tokenizer for chunking (ç”¨äºåˆ†å—çš„BERTåˆ†è¯å™¨)
* **sentence-transformers** : Embedding models (åµŒå…¥æ¨¡å‹)

---

## ğŸ“š Learning Path / å­¦ä¹ è·¯å¾„

### Beginner / åˆå­¦è€…

1. Start with **ChromaDemo** to understand vector databases (ä» ChromaDemo å¼€å§‹ä»¥ç†è§£å‘é‡æ•°æ®åº“)
2. Explore **TextParser** for basic document processing (æ¢ç´¢ TextParser äº†è§£åŸºç¡€æ–‡æ¡£å¤„ç†)
3. Try **ChunkingDemo** to experiment with chunking strategies (å°è¯• ChunkingDemo å®éªŒä¸åŒçš„åˆ†å—ç­–ç•¥)

### Intermediate / è¿›é˜¶è€…

4. Use **PDFParser** and **WordParser** for real documents (ä½¿ç”¨ PDFParser å’Œ WordParser å¤„ç†çœŸå®æ–‡æ¡£)
5. Study **BookSearch/app_v1.py** and **app_v2.py** for RAG basics (å­¦ä¹  BookSearch ç›®å½•ä¸‹çš„åº”ç”¨ä»¥æŒæ¡ RAG åŸºç¡€)
6. Review **SearchTool** notebooks for search concepts (æŸ¥é˜… SearchTool çš„ Notebooks äº†è§£æœç´¢æ¦‚å¿µ)

### Advanced / é«˜çº§å¼€å‘è€…

7. Implement **hybrid_rag.py** for production-quality search (å®ç° hybrid_rag.py ä»¥è¾¾åˆ°ç”Ÿäº§çº§åˆ«çš„æœç´¢è´¨é‡)
8. Build custom parsers combining multiple demos (ç»“åˆå¤šä¸ªæ¼”ç¤ºæ„å»ºè‡ªå®šä¹‰è§£æå™¨)
9. Scale to production with proper error handling and monitoring (æ·»åŠ é€‚å½“çš„é”™è¯¯å¤„ç†å’Œç›‘æ§ä»¥æ‰©å±•åˆ°ç”Ÿäº§ç¯å¢ƒ)

---

## ğŸ¤ Use Cases / åº”ç”¨åœºæ™¯

### Documentation Q&A / æ–‡æ¡£é—®ç­”

* **Tools / å·¥å…·** : PDFParser + BookSearch
* **Example / ç¤ºä¾‹** : Company policy documents, technical manuals (å…¬å¸æ”¿ç­–æ–‡æ¡£ã€æŠ€æœ¯æ‰‹å†Œ)

### Customer Support / å®¢æˆ·æ”¯æŒ

* **Tools / å·¥å…·** : ChromaDemo + hybrid search (æ··åˆæœç´¢)
* **Example / ç¤ºä¾‹** : FAQ database, support ticket history (å¸¸è§é—®é¢˜æ•°æ®åº“ã€æ”¯æŒå·¥å•å†å²)

### Research Assistant / ç ”ç©¶åŠ©æ‰‹

* **Tools / å·¥å…·** : PDFParser + BookSearch
* **Example / ç¤ºä¾‹** : Academic papers, research notes (å­¦æœ¯è®ºæ–‡ã€ç ”ç©¶ç¬”è®°)

### Code Documentation / ä»£ç æ–‡æ¡£

* **Tools / å·¥å…·** : TextParser + semantic search (è¯­ä¹‰æœç´¢)
* **Example / ç¤ºä¾‹** : README files, code comments (READMEæ–‡ä»¶ã€ä»£ç æ³¨é‡Š)

### Data Analysis / æ•°æ®åˆ†æ

* **Tools / å·¥å…·** : CSVParser + vector search (å‘é‡æœç´¢)
* **Example / ç¤ºä¾‹** : Customer databases, log analysis (å®¢æˆ·æ•°æ®åº“ã€æ—¥å¿—åˆ†æ)

---

## ğŸ› Troubleshooting / æ•…éšœæ’é™¤

### Common Issues / å¸¸è§é—®é¢˜

**"Module not found" errors / "æ‰¾ä¸åˆ°æ¨¡å—" é”™è¯¯**

**Bash**

```
pip install -r requirements.txt
```

**Ollama connection errors (BookSearch) / Ollamaè¿æ¥é”™è¯¯**

**Bash**

```
ollama serve  # Start Ollama server (å¯åŠ¨OllamaæœåŠ¡å™¨)
ollama pull llama3.3
ollama pull nomic-embed-text
```

**PDF extraction issues / PDFæå–é—®é¢˜**

* Scanned PDFs need OCR (pytesseract + pdf2image) (æ‰«æç‰ˆPDFéœ€è¦OCRæŠ€æœ¯)
* Password-protected PDFs not supported (ä¸æ”¯æŒå—å¯†ç ä¿æŠ¤çš„PDF)
* Try alternative: pdfplumber or PyMuPDF (å°è¯•æ›¿ä»£æ–¹æ¡ˆï¼špdfplumber æˆ– PyMuPDF)

**Memory errors with large files / å¤§æ–‡ä»¶å¯¼è‡´çš„å†…å­˜é”™è¯¯**

* Reduce chunk size (å‡å°åˆ†å—å¤§å°)
* Process files in batches (æ‰¹é‡å¤„ç†æ–‡ä»¶)
* Use streaming approaches (ä½¿ç”¨æµå¼å¤„ç†æ–¹æ³•)

**Poor search results / æœç´¢ç»“æœä¸ä½³**

* Adjust chunk size (try smaller/larger) (è°ƒæ•´åˆ†å—å¤§å°)
* Increase overlap (15-20% of chunk size) (å¢åŠ é‡å æ¯”ä¾‹)
* Use hybrid search instead of semantic only (ä½¿ç”¨æ··åˆæœç´¢æ›¿ä»£å•ä¸€çš„è¯­ä¹‰æœç´¢)

---

## ğŸ”’ Best Practices / æœ€ä½³å®è·µ

1. **Start Simple / ä»ç®€å¼€å§‹** : Begin with ChromaDemo, then add complexity (å…ˆä»ChromaDemoå¼€å§‹ï¼Œå†é€æ­¥å¢åŠ å¤æ‚åº¦)
2. **Test Chunking / æµ‹è¯•åˆ†å—** : Use ChunkingDemo to find optimal strategy (ä½¿ç”¨ChunkingDemoå¯»æ‰¾æœ€ä½³ç­–ç•¥)
3. **Version Control / ç‰ˆæœ¬æ§åˆ¶** : Track changes to chunk size and overlap (è·Ÿè¸ªåˆ†å—å¤§å°å’Œé‡å å‚æ•°çš„å˜æ›´)
4. **Monitor Quality / ç›‘æ§è´¨é‡** : Regularly evaluate retrieval accuracy (å®šæœŸè¯„ä¼°æ£€ç´¢å‡†ç¡®ç‡)
5. **Document Metadata / æ–‡æ¡£å…ƒæ•°æ®** : Always preserve source information (å§‹ç»ˆä¿ç•™æ¥æºä¿¡æ¯)
6. **Error Handling / é”™è¯¯å¤„ç†** : Wrap parsers in try-except for production (åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å°†è§£æå™¨åŒ…è£¹åœ¨try-exceptå—ä¸­)
7. **Batch Processing / æ‰¹é‡å¤„ç†** : Process multiple files efficiently (é«˜æ•ˆåœ°å¤„ç†å¤šä¸ªæ–‡ä»¶)
8. **Clean Data / æ•°æ®æ¸…æ´—** : Preprocess documents before ingestion (åœ¨æ‘„å…¥ä¹‹å‰å¯¹æ–‡æ¡£è¿›è¡Œé¢„å¤„ç†)

---

## ğŸ“– Additional Resources / é™„åŠ èµ„æº

### Related Projects / ç›¸å…³é¡¹ç›®

* [LangChain](https://github.com/langchain-ai/langchain) - RAG framework (RAGæ¡†æ¶)
* [LlamaIndex](https://github.com/run-llama/llama_index) - Data framework for LLMs (ç”¨äºå¤§æ¨¡å‹çš„æ•°æ®æ¡†æ¶)
* [ChromaDB](https://github.com/chroma-core/chroma) - Vector database (å‘é‡æ•°æ®åº“)
* [Ollama](https://github.com/ollama/ollama) - Local LLM runtime (æœ¬åœ°å¤§æ¨¡å‹è¿è¡Œç¯å¢ƒ)

### Learning Materials / å­¦ä¹ èµ„æ–™

* [RAG Explanation / RAGåŸç†è§£æ](https://www.pinecone.io/learn/retrieval-augmented-generation/)
* [Vector Database Guide / å‘é‡æ•°æ®åº“æŒ‡å—](https://www.pinecone.io/learn/vector-database/)
* [Chunking Strategies / åˆ†å—ç­–ç•¥](https://www.pinecone.io/learn/chunking-strategies/)

---

## ğŸ¤ Contributing / è´¡çŒ®æŒ‡å—

This is an educational repository. Feel free to:

è¿™æ˜¯ä¸€ä¸ªæ•™è‚²æ€§è´¨çš„ä»“åº“ã€‚æ¬¢è¿ï¼š

* Fork and modify for your projects (Forkå¹¶ä¿®æ”¹ä»¥ç”¨äºä½ è‡ªå·±çš„é¡¹ç›®)
* Report issues or suggest improvements (æŠ¥å‘Šé—®é¢˜æˆ–æå‡ºæ”¹è¿›å»ºè®®)
* Share your implementations and learnings (åˆ†äº«ä½ çš„å®ç°æ–¹æ³•å’Œå­¦ä¹ å¿ƒå¾—)

## ğŸ“ License / è®¸å¯è¯

Educational demo project. All code provided as-is for learning purposes.

æ•™è‚²æ¼”ç¤ºé¡¹ç›®ã€‚æ‰€æœ‰ä»£ç æŒ‰åŸæ ·æä¾›ï¼Œä»…ä¾›å­¦ä¹ ä½¿ç”¨ã€‚

## ğŸ™ Acknowledgments / è‡´è°¢

Sample texts in SearchFiles/ are public domain works from Project Gutenberg.

SearchFiles/ ç›®å½•ä¸­çš„æ ·ä¾‹æ–‡æœ¬æ¥è‡ªå¤è…¾å ¡è®¡åˆ’ (Project Gutenberg) çš„å…¬æœ‰é¢†åŸŸä½œå“ã€‚

---

**Happy Building! ğŸš€ / ç¥ä½ æ„å»ºæ„‰å¿«ï¼**

Start with any demo that matches your needs, or combine multiple parsers for a complete solution. Each subdirectory has detailed documentation to guide you.

ä»æ»¡è¶³ä½ éœ€æ±‚çš„ä»»ä½•æ¼”ç¤ºå¼€å§‹ï¼Œæˆ–è€…ç»“åˆå¤šä¸ªè§£æå™¨æ„å»ºå®Œæ•´çš„è§£å†³æ–¹æ¡ˆã€‚æ¯ä¸ªå­ç›®å½•éƒ½æœ‰è¯¦ç»†çš„æ–‡æ¡£æ¥æŒ‡å¯¼ä½ ã€‚

---

Would you like me to walk through the implementation details of any specific section, like the LangChain integrations or the hybrid search (BM25 + Vector) logic?
