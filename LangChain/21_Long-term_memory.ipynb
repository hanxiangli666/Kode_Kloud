{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd9c853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 开始测试：数学线程 (Session: math-thread1) ===\n",
      "User: What does cosine mean?\n",
      "AI: Cosine is a trigonometric function that represents the ratio of the adjacent side to the hypotenuse in a right triangle.\n",
      "\n",
      "User: Tell me more!\n",
      "AI: Cosine is abbreviated as cos. It's used to calculate angles and distances in trigonometry and has a periodic nature.\n",
      "\n",
      "=== 开始测试：物理线程隔离 (Session: phy-thread1) ===\n",
      "User: What is the theory of relativity?\n",
      "AI: Theory explaining how gravity affects time and space, proposed by Einstein. Includes special and general relativity.\n",
      "\n",
      "User: Tell me more!\n",
      "AI: Special relativity deals with motion at constant speed, while general relativity includes gravity's effect on spacetime.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 【硬性要求】加载环境变量，确保读取 OPENAI_API_KEY\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# 【规范更新】旧版本：from langchain_openai.chat_models import ChatOpenAI\n",
    "# 现代写法：统一直接从 langchain_openai 顶层导入\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 引入 Redis 消息历史和历史记录包装器\n",
    "from langchain_redis import RedisChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# ==========================================\n",
    "# 1. 搭建基础链 (Base Chain) \n",
    "# ==========================================\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're an assistant who's good at {ability}. Respond in 20 words or fewer.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "base_chain = prompt | model\n",
    "\n",
    "# ==========================================\n",
    "# 2. 定义记忆检索策略 (Memory Retrieval Strategy)\n",
    "# ==========================================\n",
    "REDIS_URL = \"redis://localhost:6379/0\"\n",
    "# 先启动docker 还要在终端运行以下命令启动 Redis 容器：\n",
    "# docker run -d -p 6379:6379 redis\n",
    "# 这个函数会被 RunnableWithMessageHistory 在每次链调用时动态调用\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 【科研级保姆注释】\n",
    "# 这是一个工厂函数。LangChain 每次调用链时，都会动态调用这个函数。\n",
    "# 它根据传入的 session_id，返回一个指向该会话的 Redis 连接对象。\n",
    "def get_message_history(session_id: str) -> RedisChatMessageHistory:\n",
    "    # 显式指定参数名 session_id 和 redis_url\n",
    "    return RedisChatMessageHistory(\n",
    "        session_id=session_id, \n",
    "        redis_url=REDIS_URL\n",
    "    )\n",
    "\n",
    "# ==========================================\n",
    "# 3. 组装“带有记忆的链” (Chain with Memory)\n",
    "# ==========================================\n",
    "# 使用 RunnableWithMessageHistory 将基础链和记忆工厂包装在一起\n",
    "redis_chain = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_message_history, # 传入上面定义的工厂函数句柄 (Function Reference)\n",
    "    input_messages_key=\"input\",   # 告诉它哪个变量是用户的当前输入\n",
    "    history_messages_key=\"history\" # 告诉它哪个变量用来塞入历史记录\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 4. 执行并发多租户测试 (Multi-tenant Testing)\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== 开始测试：数学线程 (Session: math-thread1) ===\")\n",
    "    \n",
    "    # 回忆上一节课的知识！session_id 是通过 configurable 动态注入的\n",
    "    config_math = {\"configurable\": {\"session_id\": \"math-thread1\"}}\n",
    "    \n",
    "    resp1 = redis_chain.invoke(\n",
    "        {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},\n",
    "        config=config_math\n",
    "    )\n",
    "    print(f\"User: What does cosine mean?\\nAI: {resp1.content}\\n\")\n",
    "    \n",
    "    resp2 = redis_chain.invoke(\n",
    "        {\"ability\": \"math\", \"input\": \"Tell me more!\"}, # 第二次调用，没有提供前置上下文\n",
    "        config=config_math\n",
    "    )\n",
    "    print(f\"User: Tell me more!\\nAI: {resp2.content}\\n\")\n",
    "    \n",
    "    print(\"=== 开始测试：物理线程隔离 (Session: phy-thread1) ===\")\n",
    "    \n",
    "    # 切换到全新的 Session ID，验证记忆是否隔离\n",
    "    config_phy = {\"configurable\": {\"session_id\": \"phy-thread1\"}}\n",
    "    \n",
    "    resp3 = redis_chain.invoke(\n",
    "        {\"ability\": \"physics\", \"input\": \"What is the theory of relativity?\"},\n",
    "        config=config_phy\n",
    "    )\n",
    "    print(f\"User: What is the theory of relativity?\\nAI: {resp3.content}\\n\")\n",
    "    \n",
    "    resp4 = redis_chain.invoke(\n",
    "        {\"ability\": \"physics\", \"input\": \"Tell me more!\"}, \n",
    "        config=config_phy\n",
    "    )\n",
    "    # AI 会继续解释相对论，而不是数学的余弦定理，这证明多租户隔离成功！\n",
    "    print(f\"User: Tell me more!\\nAI: {resp4.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5750b44",
   "metadata": {},
   "source": [
    "下面我会先用中文把“你现有这些文件夹里的技术点，哪些能直接用到老师的 Project 6”做一个映射清单；然后给你一份**3000 词以上、全英文**的技术说明稿（把你的实践代码思路与老师项目的四代理架构融合成一套可解释、可运行、可验证的实现方案）。由于你没有提供老师现有代码仓库结构，我会在英文稿里**明确写出关键假设**，并把接口/模块设计成“可嵌入 DiscoveryHub backend”的形式。\n",
    "\n",
    "**你现有代码可复用到老师项目的技术映射（中文速览）**\n",
    "- 来自 rag-project：RAG 全链路（TF‑IDF/BM25 基线→语义检索→向量库→chunking→agentic chunking），可直接用于老师项目的 **Agent 3 Industry Match Predictor**（行业/公司匹配）与部分 **Agent 2 Patent Landscape**（专利语义检索与聚类/白空间）。\n",
    "- 来自 LangChain/：Prompt 模板、Output Parser、LCEL 编排、Callbacks 调试、Memory、Retrieval chain 等，可直接用于**四个 Agent 的编排与可观测性**（透明、可审计、可复现），也能把你的实现从“脚本”升级为“可维护服务”。\n",
    "- 来自 Introduction_To_OpenAI/：chat、embeddings、structured output、batch processing、speech/image（后两者对本项目非核心但可作为扩展），可直接用于 **结构化信息抽取**（Agent 1/2）、**批量处理大量论文摘要/专利**（离线 pipeline）、以及**结构化商业简报**（Agent 4）。\n",
    "- 来自 Running_LocalLLMS_with_Ollama/：本地 LLM（隐私/成本/可控）+ 服务化封装，可用于老师强调的“production system”方向：在不泄露敏感 disclosure/未公开论文内容时，用本地模型做初步抽取/摘要，再对关键步骤用云模型做增强（可选）。\n",
    "- 来自 server.py：MCP Server 的“资源+工具+提示模板”范式，可直接迁移为老师项目的**工具层**：把 USPTO/EPO 查询、IPC 分类、公司数据库检索、联系人推荐等包装成 MCP tools，让 agent 通过标准协议调用，增强透明度与可审计性。\n",
    "\n",
    "---\n",
    "\n",
    "**English Technical Brief (3000+ words)**  \n",
    "**Integrating My Preparation Code into Project 6: DiscoveryHub AI Technology Transfer and Innovation Matchmaker**  \n",
    "**Using LLMs to Accelerate IP Licensing and Industry Partnership Identification**\n",
    "\n",
    "**1. Executive Summary**  \n",
    "This document explains how the technical work I have completed in my preparation repositories can be directly applied to, and integrated with, Project 6 – “DiscoveryHub AI Technology Transfer and Innovation Matchmaker.” The target system is an AI-powered Technology Transfer Engine embedded into DiscoveryHub’s backend, implementing a four-agent LLM architecture:\n",
    "\n",
    "- Agent 1: Research Analyzer (publication/grant ingestion, technical contribution extraction)  \n",
    "- Agent 2: Patent Landscape Mapper (prior art search, white space detection, IPC classification)  \n",
    "- Agent 3: Industry Match Predictor (retrieval-augmented alignment of innovation domains to corporate R&D focus areas; ranked partner list)  \n",
    "- Agent 4: Commercial Summary Generator (two-page commercialization brief for industry outreach)\n",
    "\n",
    "My preparation code provides reusable building blocks for each of these agents, especially in three areas that determine whether an LLM system is production-grade: (a) retrieval and indexing (RAG), (b) structured outputs and validation, and (c) tool-based orchestration with strong observability. This brief proposes a concrete integration plan, including data flow, module boundaries, output contracts, evaluation methodology, and a runbook-style approach to demonstrate the system to a supervisor.\n",
    "\n",
    "**Scope note / assumptions:** DiscoveryHub’s internal backend codebase was not provided here. Therefore, the design below is intentionally modular, exposing clear interfaces (Python modules and/or service endpoints) that can be embedded into an existing backend stack. Where relevant, I describe how components can run as scripts first (for rapid iteration) and then be promoted to services (for production integration).\n",
    "\n",
    "---\n",
    "\n",
    "**2. How My Existing Folders Map to the DiscoveryHub Project**\n",
    "\n",
    "**2.1 RAG pipeline and chunking foundations (directly supports Agents 2 and 3)**  \n",
    "The folder rag-project demonstrates the full evolution from keyword baselines to semantic retrieval and vector database indexing. The DiscoveryHub project requires exactly this capability, because partner identification and prior-art scanning are, at their core, retrieval tasks across heterogeneous corpora:\n",
    "\n",
    "- Publications and grant abstracts (Agent 1 input)  \n",
    "- Patent records and patent families (Agent 2)  \n",
    "- Industry sector databases and corporate R&D focus areas (Agent 3)  \n",
    "- DiscoveryHub’s own internal IP marketplace metadata (to be integrated)\n",
    "\n",
    "Key transferable techniques from my RAG work include:\n",
    "\n",
    "1) **Baseline retrieval (TF‑IDF/BM25) for transparency and fallback**  \n",
    "Scripts such as 02tfidf_search.py and 03bm25_search.py show classical lexical retrieval. In a university technology transfer context, lexical retrieval remains valuable for exact identifiers (e.g., specific material names, gene/protein names, product names, IPC codes, chemical formulae). It also provides a transparent baseline for evaluation (a key requirement for “auditable” systems).\n",
    "\n",
    "2) **Semantic retrieval (embeddings) for paraphrases and domain similarity**  \n",
    "07semantic_search_demo.py demonstrates meaning-based similarity search. This is essential for matching innovations to industry needs, where wording differs drastically between academic prose and corporate problem statements.\n",
    "\n",
    "3) **Vector database ingestion and retrieval loops**  \n",
    "The combination of 08init_vectordb.py, 09store_documents.py, and 10vector_search_demo.py maps to production requirements: persistent indexing, metadata storage, repeatable ingestion, and fast query-time retrieval.\n",
    "\n",
    "4) **Chunking strategy as a quality lever**  \n",
    "Scripts like 11basic_chunking.py, 12overlap_chunking.py, 13sentence_chunking.py, and 15chunked_search.py directly correspond to a major real-world issue: academic documents and patents are long, structured, and contain dense claims. High-quality chunking improves retrieval relevance, improves grounding, and reduces hallucination risk.\n",
    "\n",
    "5) **Agentic chunking for high-value documents**  \n",
    "The script 16agentic_chunking_demo.py shows LLM-assisted chunk boundary detection. In DiscoveryHub, agentic chunking can be used offline for “high stakes” inputs (e.g., high-value disclosures, core patent claims), where improved chunk coherence may substantially improve downstream matching and summarization.\n",
    "\n",
    "**2.2 LangChain orchestration, LCEL composition, and debugging (supports all four agents)**  \n",
    "The notebook series in LangChain/ provides an engineering framework to turn single-shot prompts into composable pipelines with stable interfaces, with emphasis on:\n",
    "\n",
    "- Prompt templates and consistent message structure  \n",
    "- Output parsing to avoid brittle string handling  \n",
    "- LCEL patterns for pipeline composition  \n",
    "- Callbacks and debugging for observability  \n",
    "- Memory patterns for multi-step reasoning flows  \n",
    "- Retrieval chains for RAG-based question answering\n",
    "\n",
    "DiscoveryHub’s proposal emphasizes a “transparent, auditable” multi-agent architecture validated against TTO staff assessments. This requires strong instrumentation: being able to show (1) what documents were retrieved, (2) what structured evidence was extracted, (3) which tool calls were made, and (4) why the final output was produced. LangChain’s callback and chain composition paradigms align strongly with those requirements.\n",
    "\n",
    "**2.3 OpenAI capability scripts: structured outputs and batch processing (supports scalability and evaluation)**  \n",
    "The scripts in Introduction_To_OpenAI/ cover several capabilities that map directly to the agent tasks:\n",
    "\n",
    "- structured-output.py: structured extraction, critical for Agent 1 (technical contributions), Agent 2 (patent mapping and IPC classification labels), and Agent 4 (commercialization brief sections).  \n",
    "- batch-processing.py: batch workflows are necessary to scale to many faculty outputs and many patents. Batch mode also helps evaluation (reproducible runs on a fixed dataset).  \n",
    "- embedding.py: complements the RAG folder; can be used to switch embedding backends or test alternative embedding models.\n",
    "\n",
    "**2.4 Local LLM deployment with Ollama (optional but valuable for privacy and cost control)**  \n",
    "Running_LocalLLMS_with_Ollama/ demonstrates how to run LLM inference locally and wrap it behind an application server. This is particularly relevant in technology transfer contexts where some documents may be confidential (e.g., unpublished invention disclosures). A hybrid approach becomes possible:\n",
    "\n",
    "- Use local LLMs for initial extraction/summarization and to pre-filter documents.  \n",
    "- Use hosted models selectively for higher-accuracy steps (e.g., final brief generation) where permitted.\n",
    "\n",
    "Even if DiscoveryHub ultimately uses hosted models, demonstrating a local-first pathway is a strong signal of engineering maturity and risk awareness.\n",
    "\n",
    "**2.5 MCP server pattern for tool exposure (supports auditable multi-agent architecture)**  \n",
    "The file server.py is a minimal but complete example of exposing “resources + tools + prompts” to an AI host via MCP. This pattern maps cleanly onto DiscoveryHub’s agents because Agents 2 and 3 are tool-heavy:\n",
    "\n",
    "- Patent search tools (USPTO/EPO APIs)  \n",
    "- Patent family clustering tools  \n",
    "- IPC taxonomy lookup/classification tools  \n",
    "- Company/sector database retrieval tools  \n",
    "- Ranking and scoring tools  \n",
    "- Export/report generation tools\n",
    "\n",
    "By implementing these as MCP tools, we gain:\n",
    "- Standardized tool schemas (input/output)  \n",
    "- Discoverability by the agent host  \n",
    "- Logging opportunities at the tool boundary (audit trail)  \n",
    "- Easier replacement of stubs with real APIs\n",
    "\n",
    "---\n",
    "\n",
    "**3. Proposed Integrated System Architecture for DiscoveryHub**\n",
    "\n",
    "**3.1 Data sources and internal corpora**  \n",
    "The DiscoveryHub engine must operate across multiple heterogeneous corpora:\n",
    "\n",
    "- Faculty publications (PDF, HTML, metadata records)  \n",
    "- Grant abstracts (structured text from internal systems)  \n",
    "- Patents (USPTO/EPO APIs, with claims, abstracts, CPC/IPC codes, assignees)  \n",
    "- Industry datasets (sector databases, corporate R&D focus text, press releases, product pages, job postings, 10-K sections if available)  \n",
    "- DiscoveryHub internal: existing disclosures, tags, outcomes, partner interactions\n",
    "\n",
    "**3.2 Two primary pipelines: ingestion vs query-time reasoning**  \n",
    "A production-quality RAG system splits work into two distinct pipelines:\n",
    "\n",
    "1) **Ingestion pipeline (offline / batch)**  \n",
    "   - Fetch documents  \n",
    "   - Normalize (clean, deduplicate)  \n",
    "   - Chunk  \n",
    "   - Embed  \n",
    "   - Store in a vector database with metadata  \n",
    "   - Optionally compute lexical indices (BM25) for hybrid retrieval\n",
    "\n",
    "2) **Query-time pipeline (online / interactive)**  \n",
    "   - Accept a new faculty output or disclosure  \n",
    "   - Agent 1 extracts technical contributions and domains (structured)  \n",
    "   - Agent 2 retrieves relevant patents; maps IPC; identifies white spaces  \n",
    "   - Agent 3 retrieves relevant companies; predicts partner fit and ranks  \n",
    "   - Agent 4 synthesizes the commercialization brief (structured + formatted)\n",
    "\n",
    "My scripts in rag-project represent a working prototype of the ingestion and retrieval loops, including chunking experiments. The DiscoveryHub project requires scaling these loops, adding richer metadata, and adding tool boundaries and evaluation.\n",
    "\n",
    "**3.3 Shared infrastructure components**  \n",
    "To support all agents consistently, the system should include:\n",
    "\n",
    "- **Document Store**: raw documents + normalized text + metadata  \n",
    "- **Vector Store**: embeddings for chunks (publications, patents, companies, sectors)  \n",
    "- **Lexical Index** (optional but recommended): BM25 for exact-match retrieval  \n",
    "- **Tool Gateway**: wrapper around external APIs (USPTO/EPO, industry datasets)  \n",
    "- **Orchestrator**: the agent runner that executes multi-step flows and collects traces  \n",
    "- **Trace and Audit Log**: store tool calls, retrieved evidence IDs, prompt templates, and final outputs  \n",
    "- **Evaluation Harness**: runs the system on a test set and scores against TTO rubric\n",
    "\n",
    "LangChain’s chain composition plus structured outputs provide the orchestrator skeleton; MCP provides a clean tool gateway boundary.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Agent-by-Agent Design: Interfaces, Retrieval Strategy, and Output Contracts**\n",
    "\n",
    "A major factor in “transparent, auditable” systems is forcing each agent to emit structured outputs with clear provenance. Below is a concrete design, aligned with the professor’s four agents.\n",
    "\n",
    "### **4.1 Agent 1: Research Analyzer (ingestion + structured extraction)**\n",
    "\n",
    "**Goal:** Ingest a publication/grant abstract and extract:\n",
    "- Key technical contributions (what is new?)  \n",
    "- Methodological novelties (how is it done?)  \n",
    "- Application domains (where can it be used?)  \n",
    "- Keywords and entities (materials, methods, targets)  \n",
    "- A short “innovation claim” summary suitable for downstream retrieval\n",
    "\n",
    "**How my work applies:**  \n",
    "- Prompting and message design techniques from 05_Prompt_templates_demo.ipynb and 06_Few-shot_prompt_templates_demo.ipynb help produce consistent extraction.  \n",
    "- Structured output techniques from structured-output.py reduce ambiguity and make the agent auditable.  \n",
    "- Batch processing patterns from batch-processing.py allow scaling across many faculty outputs.\n",
    "\n",
    "**Recommended output schema (example):**  \n",
    "- `innovation_title` (string)  \n",
    "- `technical_summary` (string)  \n",
    "- `novel_contributions` (list of strings)  \n",
    "- `methods` (list of strings)  \n",
    "- `application_domains` (list of strings)  \n",
    "- `keywords` (list of strings)  \n",
    "- `risk_notes` (list of strings; e.g., “insufficient detail,” “possible overlap with known methods”)  \n",
    "- `source_refs` (document id, section offsets)\n",
    "\n",
    "**Auditing principle:** Every extracted bullet should be traceable to a source snippet (document id + chunk ids). This is where chunking quality matters.\n",
    "\n",
    "### **4.2 Agent 2: Patent Landscape Mapper (retrieval + taxonomy + whitespace)**\n",
    "\n",
    "**Goal:** Identify relevant prior art and patent families; classify the innovation against IPC taxonomy; identify white spaces (areas with limited prior art or weak coverage).\n",
    "\n",
    "**How my work applies:**  \n",
    "- RAG retrieval across patent abstracts and claims is directly supported by the indexing and retrieval patterns in 08init_vectordb.py through 10vector_search_demo.py.  \n",
    "- Hybrid retrieval is important: lexical retrieval catches exact IPC codes and named entities; semantic retrieval catches paraphrases. My baseline scripts in 02tfidf_search.py and 03bm25_search.py provide the conceptual foundation.  \n",
    "- Agentic chunking from 16agentic_chunking_demo.py can be used to split patent claims into semantically coherent segments for better mapping.\n",
    "\n",
    "**Tooling via MCP (recommended):**  \n",
    "Implement patent interactions as MCP tools, similar to server.py:\n",
    "- `search_patents(query, jurisdictions, date_range)`  \n",
    "- `get_patent_details(patent_id)`  \n",
    "- `map_to_ipc(text)`  \n",
    "- `cluster_patent_family(patent_ids)`  \n",
    "- `compute_whitespace(innovation_embedding, patent_embeddings)`\n",
    "\n",
    "Each tool call becomes a loggable, testable unit.\n",
    "\n",
    "**Recommended output schema:**  \n",
    "- `related_patents` (list of objects: id, title, assignee, year, similarity, url)  \n",
    "- `patent_families` (grouped ids)  \n",
    "- `ipc_classification` (list of IPC codes + confidence + evidence chunks)  \n",
    "- `whitespace_hypotheses` (list of claims about gaps + supporting rationale)  \n",
    "- `prior_art_risks` (list of specific overlaps and why they matter)\n",
    "\n",
    "**Auditing principle:** Patent matches must include the retrieved chunk ids and similarity scores. White space claims must cite what corpus was searched and what thresholds were used.\n",
    "\n",
    "### **4.3 Agent 3: Industry Match Predictor (RAG + ranking)**\n",
    "\n",
    "**Goal:** Align innovation application domains to industry sectors and corporate R&D priorities; produce a ranked list of potential partners with reasoning.\n",
    "\n",
    "**How my work applies:**  \n",
    "This is the most direct reuse of my RAG prep:\n",
    "- The chunked retrieval and chunking comparisons in 15chunked_search.py mirror the matching problem: your “query” is the innovation profile; your “documents” are company focus descriptions.  \n",
    "- Semantic embeddings and vector search (steps 07–10) map precisely to “find companies whose R&D language matches the innovation.”  \n",
    "- Hybrid retrieval supports company name exact matches and acronym expansion.\n",
    "\n",
    "**Ranking strategy (recommended):**  \n",
    "A robust partner ranking is not “LLM intuition.” It should combine:\n",
    "- Vector similarity between innovation embedding and company R&D embeddings  \n",
    "- Lexical matches on keywords and named entities  \n",
    "- Sector compatibility (taxonomy-based features)  \n",
    "- Historical success signals (if DiscoveryHub has outcome data)  \n",
    "- Penalties for weak evidence (few retrieved chunks, low similarity dispersion)\n",
    "\n",
    "The LLM can then explain the ranking using the evidence, but the ranking itself should be at least partially algorithmic. This increases auditability and reduces variability.\n",
    "\n",
    "**Recommended output schema:**  \n",
    "- `ranked_partners` (list of objects: company_id, company_name, score, evidence_chunks, rationale)  \n",
    "- `sector_alignment` (which sectors and why)  \n",
    "- `risk_flags` (e.g., “evidence from marketing pages only,” “limited R&D evidence”)  \n",
    "- `contact_suggestions` (if available; otherwise placeholders)\n",
    "\n",
    "### **4.4 Agent 4: Commercial Summary Generator (structured synthesis + formatting)**\n",
    "\n",
    "**Goal:** Produce a two-page commercialization brief:\n",
    "- Plain-language technology description  \n",
    "- Differentiators  \n",
    "- Market opportunity summary  \n",
    "- Preliminary licensing framework  \n",
    "- Format suitable for distribution to industry contacts\n",
    "\n",
    "**How my work applies:**  \n",
    "- Structured output and formatting discipline from structured-output.py ensures the brief is consistent and scorable by a rubric.  \n",
    "- Prompt template and few-shot techniques from 05_Prompt_templates_demo.ipynb and 06_Few-shot_prompt_templates_demo.ipynb help produce “industry-facing” language rather than academic prose.  \n",
    "- Callback/trace techniques from 12_StdOutCallbackHandler_demo.ipynb support transparency: the brief should reference which extracted fields and which evidence were used.\n",
    "\n",
    "**Recommended output schema (sectioned):**  \n",
    "- `plain_language_description`  \n",
    "- `technical_differentiators` (bullets)  \n",
    "- `target_industries` (bullets + justification)  \n",
    "- `partner_shortlist` (top N + why)  \n",
    "- `market_opportunity` (summary)  \n",
    "- `licensing_framework` (non-legal, preliminary)  \n",
    "- `evidence_appendix` (list of cited chunks and sources)\n",
    "\n",
    "**Auditing principle:** The brief must include an appendix listing the sources used (patent IDs, publication chunks, company evidence chunks). This is the simplest, most convincing way to demonstrate transparency.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Orchestration and Observability: Making a Multi-Agent System Auditable**\n",
    "\n",
    "The professor’s proposal highlights transparency and validation against real outcomes. Technically, this is achieved through:\n",
    "\n",
    "1) **Deterministic, structured outputs**  \n",
    "   Agents should emit JSON-like objects with explicit fields rather than prose. This reduces ambiguity and enables scoring.\n",
    "\n",
    "2) **Tool boundaries and traces**  \n",
    "   Every external call (patent search, company search) should go through a tool layer. MCP is a strong candidate because it standardizes tool registration and invocation patterns, as demonstrated by server.py.\n",
    "\n",
    "3) **Evidence-first RAG**  \n",
    "   Agent outputs should include evidence pointers (chunk IDs, source URLs). This matches the “auditable” requirement and also improves trust with TTO staff.\n",
    "\n",
    "4) **Callback logs**  \n",
    "   LangChain callback patterns (see 12_StdOutCallbackHandler_demo.ipynb) provide a mechanism to capture intermediate prompts, retrieved docs, and tool calls for debugging and for audit.\n",
    "\n",
    "5) **Evaluation harness integration**  \n",
    "   Batch processing patterns from batch-processing.py can be adapted to run the full 4-agent pipeline over a fixed dataset and generate a reproducible report.\n",
    "\n",
    "---\n",
    "\n",
    "**6. Privacy, Security, and Compliance Considerations**\n",
    "\n",
    "Technology transfer workflows frequently involve sensitive information:\n",
    "- unpublished disclosures  \n",
    "- early-stage research details  \n",
    "- potential partner targeting strategies\n",
    "\n",
    "My preparation work already reflects a bias toward secure practices:\n",
    "- use of `.env` and `.gitignore` across multiple folders, including .env and .env  \n",
    "- avoiding printing secrets in logs  \n",
    "- optional local model execution via Running_LocalLLMS_with_Ollama/\n",
    "\n",
    "**Recommended security posture for DiscoveryHub integration:**\n",
    "- Default to running the pipeline in a controlled backend environment (not a public notebook).  \n",
    "- Maintain strict logging rules: never log raw confidential documents; log only chunk IDs and hashes.  \n",
    "- Consider local LLMs (Ollama) for early steps, with an option to switch to hosted models under approved data-sharing policies.  \n",
    "- Implement access controls and audit logging for tool calls, especially those that query external APIs.\n",
    "\n",
    "---\n",
    "\n",
    "**7. Validation Plan: Rubrics, Metrics, and Downstream Impact**\n",
    "\n",
    "The proposal explicitly mentions validation against TTO staff assessments and measuring downstream impact. A rigorous plan should include:\n",
    "\n",
    "**7.1 Offline rubric-based evaluation (quality and correctness)**  \n",
    "For a representative sample of disclosures:\n",
    "- TTO staff provide a rubric score for clarity, accuracy, differentiator quality, and partner relevance.  \n",
    "- The system generates the same brief and is scored against the rubric.\n",
    "\n",
    "Because the agents emit structured outputs, we can also define automated checks:\n",
    "- completeness (all fields present)  \n",
    "- evidence coverage (each claim has a source)  \n",
    "- retrieval sanity (top-k similarity thresholds, document diversity)\n",
    "\n",
    "**7.2 Retrieval evaluation for Agents 2 and 3**  \n",
    "Measure:\n",
    "- how often known relevant patents appear in top-k  \n",
    "- how often relevant partners appear in top-k  \n",
    "- sensitivity to chunking strategy (basic vs overlap vs sentence vs agentic)\n",
    "\n",
    "This is where my chunking experiments from 11basic_chunking.py through 15chunked_search.py become directly useful.\n",
    "\n",
    "**7.3 Online impact evaluation (partner inquiry rates)**  \n",
    "Once embedded into DiscoveryHub:\n",
    "- track inquiry rates or conversion steps for disclosures processed via the engine vs historical baseline, as described in the professor’s proposal.  \n",
    "- ensure confounders are controlled (time windows, categories).\n",
    "\n",
    "---\n",
    "\n",
    "**8. Practical Runbook: How I Would Demonstrate and Run the System for the Supervisor**\n",
    "\n",
    "Because the professor asked me to “explain and run code,” the demonstration should be staged and repeatable:\n",
    "\n",
    "**Stage A: Show retrieval foundation works (RAG readiness)**  \n",
    "- Run the environment check and core RAG scripts as proof of readiness:\n",
    "  - 01verify_environment.py  \n",
    "  - 07semantic_search_demo.py  \n",
    "  - 10vector_search_demo.py  \n",
    "  - 15chunked_search.py\n",
    "\n",
    "**Stage B: Show structured extraction and batch capability**  \n",
    "- Run structured-output.py to demonstrate schema-bound extraction.  \n",
    "- Run batch-processing.py to show scalable processing patterns.\n",
    "\n",
    "**Stage C: Show orchestration patterns**  \n",
    "- Walk through prompt templates and parsers in 05_Prompt_templates_demo.ipynb and 03_model_io_parser.ipynb.  \n",
    "- Demonstrate callbacks for traceability using 12_StdOutCallbackHandler_demo.ipynb.\n",
    "\n",
    "**Stage D: Show tool server boundary (MCP) for auditable tool use**  \n",
    "- Run server.py as a proof-of-concept of how a tool server would look.  \n",
    "- Explain how the same pattern would expose `search_patents` and `search_companies` tools with deterministic outputs and logs.\n",
    "\n",
    "**Stage E (optional): Show local model fallback for sensitive data**  \n",
    "- Demonstrate local runtime conceptually via Running_LocalLLMS_with_Ollama/ to show privacy-aware engineering options.\n",
    "\n",
    "This staged demonstration is persuasive because it shows each necessary engineering capability independently before combining them.\n",
    "\n",
    "---\n",
    "\n",
    "**9. Integration Strategy: From My Prep Code to DiscoveryHub Production**\n",
    "\n",
    "**Phase 1: Build a “reference pipeline” as scripts**  \n",
    "Create a minimal set of Python modules that implement:\n",
    "- `agent1_extract()` returning structured innovation profile  \n",
    "- `agent2_patent_map()` returning patent landscape outputs  \n",
    "- `agent3_partner_match()` returning ranked partners with evidence  \n",
    "- `agent4_generate_brief()` returning a formatted two-page brief\n",
    "\n",
    "At this phase, scripts can reuse patterns and code from:\n",
    "- utils.py for loading and chunking helpers  \n",
    "- the structured output patterns from structured-output.py  \n",
    "- LangChain prompt and parser patterns from LangChain/\n",
    "\n",
    "**Phase 2: Add MCP tool servers for external APIs**  \n",
    "Implement tools as MCP endpoints:\n",
    "- Patent API tools (USPTO/EPO)  \n",
    "- Industry database tools  \n",
    "- IPC taxonomy lookup tool\n",
    "\n",
    "This uses the same “tool registration” pattern as server.py, but with real API integrations.\n",
    "\n",
    "**Phase 3: Embed into DiscoveryHub backend**  \n",
    "Once tools and agents are stable, integrate via:\n",
    "- internal service calls (if DiscoveryHub is Python-based)  \n",
    "- or HTTP endpoints (if DiscoveryHub is polyglot)\n",
    "\n",
    "**Phase 4: Evaluation and rollout**  \n",
    "Add:\n",
    "- a batch evaluation harness (adapt batch-processing.py)  \n",
    "- a rubric scoring dataset  \n",
    "- logging dashboards or trace exports\n",
    "\n",
    "---\n",
    "\n",
    "**10. Closing Statement**\n",
    "\n",
    "My preparation repositories are not isolated “toy demos.” They collectively form a coherent technical foundation for the DiscoveryHub Technology Transfer Engine:\n",
    "\n",
    "- The rag-project folder provides retrieval, vector indexing, and chunking strategies—the core of partner identification and patent mapping.  \n",
    "- The LangChain/ folder provides orchestration, structured prompting, LCEL composition, and observability—requirements for multi-agent transparency and auditability.  \n",
    "- The Introduction_To_OpenAI/ folder provides structured output and batch processing patterns—essential for correctness and scalability across many disclosures.  \n",
    "- The Running_LocalLLMS_with_Ollama/ folder provides a local inference option—useful for privacy-sensitive workflows.  \n",
    "- The server.py example provides a clean pattern for tool exposure and standardized agent tooling—crucial for integrating external patent and industry data sources.\n",
    "\n",
    "Together, these components can be combined into a rigorous, production-oriented implementation of the professor’s four-agent architecture, with strong evidence tracking, consistent structured outputs, and a clear evaluation pathway aligned with TTO staff assessments and downstream partner inquiry metrics.\n",
    "\n",
    "---\n",
    "\n",
    "如果你愿意，我下一步可以把这份英文稿再“对齐老师的口味”做两件事（不需要你给老师代码也能先做一版）：\n",
    "- 把上面每个 Agent 的输出 schema 写成更正式的“API Contract”段落（字段、类型、示例 JSON、错误码策略）。\n",
    "- 给你一套 10 分钟演示脚本（你对老师逐段讲什么、先跑哪个文件、预期输出是什么、怎么解释“透明可审计”）。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
