{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b83cd1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. æ­£åœ¨åˆå§‹åŒ–æ•°æ®æ‘„å–...\n",
      "2. æ­£åœ¨è¿›è¡Œå‘é‡åŒ–å¹¶å­˜å…¥æ•°æ®åº“...\n",
      "3. æ­£åœ¨ç»„è£… LCEL é—®ç­”ç®¡é“...\n",
      "\n",
      "=== Web-RAG ç³»ç»Ÿå·²ä¸Šçº¿ ===\n",
      "\n",
      "ğŸ™‹ æé—®: What's the size of the largest Llama 3 model?\n",
      "ğŸ¤– å›ç­”: The largest Llama 3 model will have over 400 billion parameters.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# ã€ç¡¬æ€§è¦æ±‚ã€‘åŠ è½½ç¯å¢ƒå˜é‡\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# å¼•å…¥æ ¸å¿ƒæ¨¡å—\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ==========================================\n",
    "# 1. ç´¢å¼•é˜¶æ®µï¼šæ•°æ®æºçš„çƒ­æ’æ‹” (Hot-swapping Data Sources)\n",
    "# ==========================================\n",
    "print(\"1. æ­£åœ¨åˆå§‹åŒ–æ•°æ®æ‘„å–...\")\n",
    "\n",
    "# ã€åŸä»£ç  - å·²æ³¨é‡Šã€‘ä½¿ç”¨ PDF åŠ è½½å™¨\n",
    "# loader = PyPDFLoader(\"data/handbook.pdf\")\n",
    "\n",
    "# ã€æ–°ä»£ç  - æ— ç¼æ¥å…¥ã€‘ä½¿ç”¨ç½‘é¡µåŠ è½½å™¨ï¼Œè¯»å– Llama 3 çš„æœ€æ–°æ–°é—»\n",
    "URL = \"https://www.theverge.com/2024/4/18/24133808/meta-ai-assistant-llama-3-chatgpt-openai-rival\"\n",
    "loader = WebBaseLoader(URL)\n",
    "\n",
    "# åç»­çš„æ‰€æœ‰ ETL æµç¨‹å®Œå…¨ä¸ç”¨æ”¹ï¼\n",
    "pages = loader.load_and_split()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "print(\"2. æ­£åœ¨è¿›è¡Œå‘é‡åŒ–å¹¶å­˜å…¥æ•°æ®åº“...\")\n",
    "# åˆå§‹åŒ– Embeddings æ¨¡å‹å¹¶å»ºç«‹å‘é‡æ•°æ®åº“\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings)\n",
    "\n",
    "# ==========================================\n",
    "# 2. æ£€ç´¢ä¸ç”Ÿæˆç»„ä»¶è®¾ç½® (å®Œå…¨ä¿æŒä¸å˜)\n",
    "# ==========================================\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "def format_docs(retrieved_docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    \n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "template = \"\"\"SYSTEM: You are a question answer bot.\n",
    "Be factual in your response.\n",
    "Respond to the following question ONLY from the below context: \n",
    "{context}\n",
    "\n",
    "If you don't know the answer, just say that you don't know.\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# ==========================================\n",
    "# 3. ç»„è£… LCEL ç®¡é“ (å®Œå…¨ä¿æŒä¸å˜)\n",
    "# ==========================================\n",
    "print(\"3. æ­£åœ¨ç»„è£… LCEL é—®ç­”ç®¡é“...\")\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 4. é’ˆå¯¹æ–°æ•°æ®æºæ‰§è¡Œæµ‹è¯•\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n=== Web-RAG ç³»ç»Ÿå·²ä¸Šçº¿ ===\")\n",
    "    \n",
    "    # æå‡ºé’ˆå¯¹ç½‘é¡µå†…å®¹çš„æ–°é—®é¢˜ï¼šæœ€å¤§çš„ Llama 3 æ¨¡å‹æœ‰å¤šå¤§ï¼Ÿ\n",
    "    test_question = \"What's the size of the largest Llama 3 model?\"\n",
    "    print(f\"\\nğŸ™‹ æé—®: {test_question}\")\n",
    "    \n",
    "    response = rag_chain.invoke(test_question)\n",
    "    # AI æˆåŠŸä»å®æ—¶ç½‘é¡µä¸­æŠ“å–åˆ°äº†ç­”æ¡ˆï¼šè¶…è¿‡ 4000 äº¿ä¸ªå‚æ•°\n",
    "    print(f\"ğŸ¤– å›ç­”: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
