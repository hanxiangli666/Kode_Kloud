{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "772c9bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. æ­£åœ¨æŠ“å–ç½‘é¡µå¹¶åˆ‡å—...\n",
      "2. æ­£åœ¨è¿›è¡Œå‘é‡åŒ–å¹¶å­˜å…¥ FAISS å†…å­˜æ•°æ®åº“...\n",
      "3. æ­£åœ¨ç»„è£…é«˜çº§æ£€ç´¢é“¾...\n",
      "\n",
      "ğŸ™‹ æé—®: List the models and their token size of models only from Anthropic and Meta\n",
      "\n",
      "ğŸ¤– æœ€ç»ˆå›ç­”:\n",
      "From the context provided, the models and their token sizes are:\n",
      "\n",
      "1. Anthropic's Claude 3:\n",
      "   - Initial support for a 200,000-token context window\n",
      "   - Select customers getting up to a 1-million-token context window (~700,000 words)\n",
      "\n",
      "2. Meta's Llama 2:\n",
      "   - ~4,000-token context window\n",
      "\n",
      "I don't know about the token size of models from Anthropic and Meta other than the ones mentioned above.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# ã€ç¡¬æ€§è¦æ±‚ã€‘åŠ è½½ç¯å¢ƒå˜é‡ (ç¡®ä¿å¯ä»¥è°ƒç”¨ OpenAI API)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# å¯¼å…¥é¢„æ„å»ºé“¾å·¥å‚å‡½æ•°å’Œ FAISS å‘é‡åº“\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# ==========================================\n",
    "# 1. æ•°æ®æ‘„å–ä¸ç´¢å¼• (Data Ingestion & Indexing)\n",
    "# ==========================================\n",
    "# ä¾ç„¶æ˜¯é‚£ä¸¤ç¯‡æ–°é—»ï¼šä¸€ç¯‡å…³äº Anthropic Claude 3ï¼Œä¸€ç¯‡å…³äº AI21 Labs\n",
    "URL1 = \"https://techcrunch.com/2024/03/04/anthropic-claims-its-new-models-beat-gpt-4/\"\n",
    "URL2 = \"https://techcrunch.com/2024/03/28/ai21-labs-new-text-generating-ai-model-is-more-efficient-than-most/\"\n",
    "\n",
    "print(\"1. æ­£åœ¨æŠ“å–ç½‘é¡µå¹¶åˆ‡å—...\")\n",
    "loader = WebBaseLoader([URL1, URL2])\n",
    "data = loader.load()\n",
    "\n",
    "# æ–‡æœ¬åˆ†å—\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "\n",
    "print(\"2. æ­£åœ¨è¿›è¡Œå‘é‡åŒ–å¹¶å­˜å…¥ FAISS å†…å­˜æ•°æ®åº“...\")\n",
    "# ä½¿ç”¨ FAISS æ›¿ä»£ Chroma è¿›è¡Œå†…å­˜çº§å¿«é€Ÿç´¢å¼•\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vector = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vector.as_retriever() # è½¬åŒ–ä¸ºæ£€ç´¢å™¨ç»„ä»¶\n",
    "\n",
    "# ==========================================\n",
    "# 2. æ„å»ºé¢„æ„å»ºé“¾ (Building Pre-built Chains)\n",
    "# ==========================================\n",
    "print(\"3. æ­£åœ¨ç»„è£…é«˜çº§æ£€ç´¢é“¾...\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0) # æ¸©åº¦è®¾ä¸º0é˜²æ­¢æ¨¡å‹è‡ªè¡Œå‘æŒ¥\n",
    "\n",
    "# å®šä¹‰æç¤ºè¯ã€‚æ³¨æ„ï¼šé¢„æ„å»ºé“¾é»˜è®¤æœŸæœ›ç”¨æˆ·çš„é—®é¢˜å˜é‡åä¸º {input}ï¼Œä¸Šä¸‹æ–‡å˜é‡åä¸º {context}\n",
    "prompt_template = \"\"\"\n",
    "Answer the question {input} based solely on the context below:\n",
    "\\n\\n<context>\\n{context}\\n</context>\n",
    "If you can't find an answer, say I don't know.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# æ­¥éª¤ A: åˆ›å»ºæ–‡æ¡£å¤„ç†å†…éƒ¨é“¾ (è´Ÿè´£ç”Ÿæˆç­”æ¡ˆ)\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# æ­¥éª¤ B: åˆ›å»ºå¤–å±‚æ£€ç´¢é“¾ (è´Ÿè´£å…ˆæ£€ç´¢ï¼Œå†å°†ç»“æœä¼ ç»™æ­¥éª¤ A)\n",
    "# è¿™å®Œç¾æ›¿ä»£äº†æˆ‘ä»¬ä¹‹å‰å†™çš„ {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "rag_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "# ==========================================\n",
    "# 3. æ‰§è¡Œæé—® (Execution)\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # æé—®ï¼šä»…æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ï¼Œåˆ—å‡º Anthropic å’Œ Meta æ¨¡å‹åŠå…¶ Token å¤§å°\n",
    "    question = \"List the models and their token size of models only from Anthropic and Meta\"\n",
    "    print(f\"\\nğŸ™‹ æé—®: {question}\")\n",
    "    \n",
    "    # é¢„æ„å»ºçš„æ£€ç´¢é“¾ invoke æ—¶ï¼Œä¼šè‡ªåŠ¨è¿”å›ä¸€ä¸ªåŒ…å«å¤šä¸ªé”®çš„å­—å…¸ï¼Œ\n",
    "    # çœŸæ­£çš„ç­”æ¡ˆæ–‡æœ¬åœ¨ 'answer' é”®ä¸­\n",
    "    result = rag_chain.invoke({\"input\": question})\n",
    "    \n",
    "    print(\"\\nğŸ¤– æœ€ç»ˆå›ç­”:\")\n",
    "    print(result['answer']) # è¾“å‡ºé¢„æœŸï¼šClaude 3 æ˜¯ 200,000ï¼ŒLlama 2 æ˜¯ 32,000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
