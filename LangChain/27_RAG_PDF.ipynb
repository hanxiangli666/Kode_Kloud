{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74f0fc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. æ­£åœ¨åŠ è½½ PDF æ–‡ä»¶: data/handbook.pdf ...\n",
      "2. æ­£åœ¨è¿›è¡Œå‘é‡åŒ–å¹¶å­˜å…¥ Chroma æ•°æ®åº“...\n",
      "3. æ­£åœ¨ç»„è£… LCEL ç«¯åˆ°ç«¯é—®ç­”ç®¡é“...\n",
      "\n",
      "=== RAG ç³»ç»Ÿå·²ä¸Šçº¿ ===\n",
      "\n",
      "ğŸ™‹ æé—®: According to the author, in the contemporary digital landscape, the \"datasphere\" is expanding at what rate?\n",
      "ğŸ¤– å›ç­”: The \"datasphere\" is expanding at an unprecedented rate.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# ã€ç¡¬æ€§è¦æ±‚ã€‘æ³¨å…¥ç¯å¢ƒå˜é‡ (ç¡®ä¿è¯»å–åˆ° OPENAI_API_KEY)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# å¼•å…¥æ ¸å¿ƒæ¨¡å—\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ==========================================\n",
    "# 1. ç´¢å¼•é˜¶æ®µ (Indexing Phase)\n",
    "# ==========================================\n",
    "# æŒ‡å‘ä½ çš„ç›®æ ‡ PDF æ–‡ä»¶è·¯å¾„\n",
    "PDF_PATH = \"data/handbook.pdf\"\n",
    "\n",
    "print(f\"1. æ­£åœ¨åŠ è½½ PDF æ–‡ä»¶: {PDF_PATH} ...\")\n",
    "try:\n",
    "    # å®ä¾‹åŒ–åŠ è½½å™¨å¹¶æŒ‰é¡µç²—ç•¥åˆ†å‰²\n",
    "    loader = PyPDFLoader(PDF_PATH)\n",
    "    pages = loader.load_and_split()\n",
    "    \n",
    "    # ç»†ç²’åº¦æ–‡æœ¬åˆ†å— (Chunking)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "    chunks = text_splitter.split_documents(pages)\n",
    "    \n",
    "    print(\"2. æ­£åœ¨è¿›è¡Œå‘é‡åŒ–å¹¶å­˜å…¥ Chroma æ•°æ®åº“...\")\n",
    "    # åˆå§‹åŒ– Embeddings æ¨¡å‹å¹¶å»ºç«‹å‘é‡æ•°æ®åº“\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings)\n",
    "    \n",
    "    # ==========================================\n",
    "    # 2. æ£€ç´¢ä¸ç”Ÿæˆç»„ä»¶è®¾ç½® (Retrieval & Generation Setup)\n",
    "    # ==========================================\n",
    "    # å°†å‘é‡åº“è½¬åŒ–ä¸ºæ£€ç´¢å™¨ (Retriever)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    \n",
    "    # å®šä¹‰è¾…åŠ©å‡½æ•°ï¼šæ‹¼æ¥æ£€ç´¢åˆ°çš„æ–‡æ¡£å—ä¸ºçº¯æ–‡æœ¬å­—ç¬¦ä¸²\n",
    "    def format_docs(retrieved_docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "        \n",
    "    # åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹ (LLM)\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    \n",
    "    # å®šä¹‰é˜²å¹»è§‰æç¤ºè¯æ¨¡æ¿ (Prompt Template)\n",
    "    template = \"\"\"SYSTEM: You are a question answer bot.\n",
    "    Be factual in your response.\n",
    "    Respond to the following question ONLY from the below context: \n",
    "    {context}\n",
    "    \n",
    "    If you don't know the answer, just say that you don't know.\n",
    "    \n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "    \n",
    "    # ==========================================\n",
    "    # 3. ç»„è£… LCEL ç»ˆæç®¡é“ (Assembling the Chain)\n",
    "    # ==========================================\n",
    "    print(\"3. æ­£åœ¨ç»„è£… LCEL ç«¯åˆ°ç«¯é—®ç­”ç®¡é“...\")\n",
    "    rag_chain = (\n",
    "        # é­”æ³•æ—¶åˆ»ï¼šå¹¶è¡Œå¤„ç† context å’Œ question\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # ==========================================\n",
    "    # 4. æ‰§è¡Œæµ‹è¯• (Execution)\n",
    "    # ==========================================\n",
    "    print(\"\\n=== RAG ç³»ç»Ÿå·²ä¸Šçº¿ ===\")\n",
    "    \n",
    "    # æµ‹è¯•è€å¸ˆè®²ç¨¿ä¸­çš„ç»å…¸é—®é¢˜\n",
    "    test_question = \"According to the author, in the contemporary digital landscape, the \\\"datasphere\\\" is expanding at what rate?\"\n",
    "    print(f\"\\nğŸ™‹ æé—®: {test_question}\")\n",
    "    \n",
    "    response = rag_chain.invoke(test_question)\n",
    "    print(f\"ğŸ¤– å›ç­”: {response}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nâŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ '{PDF_PATH}'ã€‚è¯·ç¡®ä¿åœ¨å½“å‰ç›®å½•ä¸‹åˆ›å»ºäº† 'data' æ–‡ä»¶å¤¹ï¼Œå¹¶æ”¾å…¥äº†åä¸º 'handbook.pdf' çš„æ–‡ä»¶ã€‚\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ è¿è¡Œå‘ç”Ÿå¼‚å¸¸: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
