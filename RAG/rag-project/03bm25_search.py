#!/usr/bin/env python3
# 1) è¯¥è„šæœ¬æ¼”ç¤ºåŸºäº BM25 çš„ä¼ ç»Ÿè¯é¡¹æ£€ç´¢; This script demonstrates classic BM25 term-based retrieval.
# 2) å®ƒå®ç°äº†æ–‡æ¡£åˆ†è¯ã€å»ºç´¢å¼•å¹¶å¯¹æŸ¥è¯¢è¯„åˆ†; It implements tokenization, index scoring, and result ranking.
# 3) ä½¿ç”¨çš„ AI æŠ€æœ¯ä¸ºä¿¡æ¯æ£€ç´¢ç®—æ³• BM25ï¼Œå±äºéè¯­ä¹‰æ£€ç´¢åŸºçº¿; The AI-related technique is BM25 IR, a non-semantic baseline.
# 4) åœ¨æœ¬ç›®å½•ä¸­ï¼Œå®ƒç”¨äºå¯¹æ¯”å‘é‡æ£€ç´¢ä¸æ··åˆæ£€ç´¢çš„æ•ˆæœ; In this folder, it is a comparison point for vector and hybrid search.
# 5) å®ƒä¸ TF-IDF å’Œè¯­ä¹‰æ£€ç´¢è„šæœ¬å…±åŒæ„æˆä¼ ç»Ÿæ£€ç´¢å¯¹ç…§ç»„; It forms the traditional-retrieval control group with TF-IDF and keyword demos.
"""
Simple BM25 Search Demo
"""

from rank_bm25 import BM25Okapi
import re
from utils import get_doc_info

# å¯åŠ¨æç¤º / Startup banner
print("ğŸ” BM25 Search Demo")
print("=" * 50)

# è¯»å–æ–‡æ¡£ / Load documents from techcorp-docs
docs, doc_paths = get_doc_info()
print(f"ğŸ“š Loaded {len(docs)} documents\n")

# æ–‡æ¡£åˆ†è¯ / Tokenize documents
tokenized_docs = [re.sub(r'[^a-zA-Z\s]', '', doc.lower()).split() for doc in docs]

# åˆ›å»º BM25 ç´¢å¼• / Create BM25 index
bm25 = BM25Okapi(tokenized_docs)

# ç¤ºä¾‹æŸ¥è¯¢ / Example searches
queries = ["remote work policy", "health insurance benefits", "pet policy dogs"]

for query in queries:
    print(f"ğŸ” Searching for: '{query}'")
    
    # æŸ¥è¯¢åˆ†è¯ / Tokenize query
    tokenized_query = re.sub(r'[^a-zA-Z\s]', '', query.lower()).split()
    
    # è®¡ç®— BM25 åˆ†æ•° / Get BM25 scores
    scores = bm25.get_scores(tokenized_query)
    
    # å–æœ€é«˜åˆ†ç»“æœ / Get top results
    top_indices = scores.argsort()[-3:][::-1]
    
    print("Results:")
    for i, idx in enumerate(top_indices, 1):
        # ä»…æ˜¾ç¤ºæ–‡ä»¶åå’Œåˆ†æ•° / Show only filename and score
        doc_name = doc_paths[idx].split('/')[-1]  # Just the filename
        print(f"  {i}. Score: {scores[idx]:.4f} - {doc_name}")
    print()

# å®Œæˆæç¤º / Completion banner
print("âœ… BM25 search completed!")