#!/usr/bin/env python3
# 1) è¯¥è„šæœ¬æ¼”ç¤ºè®©å¤§æ¨¡å‹æŒ‰è¯­ä¹‰åˆ†æ®µçš„æ™ºèƒ½åˆ‡åˆ†æµç¨‹; This script demonstrates LLM-guided semantic chunking.
# 2) å®ƒå®ç°äº†æç¤ºé©±åŠ¨çš„ä¸»é¢˜åˆ‡åˆ†å¹¶ä¸åŸºç¡€å­—ç¬¦åˆ‡åˆ†åšå¯¹æ¯”; It implements prompt-driven topic splitting and contrasts it with basic chunking.
# 3) ä½¿ç”¨çš„ AI æŠ€æœ¯åŒ…æ‹¬ LLM æ¨ç†ä¸æ–‡æœ¬åˆ‡åˆ†å™¨è”åŠ¨; AI techniques used include LLM reasoning paired with a text splitter.
# 4) åœ¨æœ¬ç›®å½•çš„å­¦ä¹ é“¾è·¯ä¸­ï¼Œå®ƒæ˜¯é«˜çº§åˆ‡åˆ†çš„é˜¶æ®µæ€§æ€»ç»“; In the learning sequence, it serves as the advanced chunking milestone.
# 5) å®ƒä¸å…¶å®ƒè„šæœ¬çš„å…³ç³»æ˜¯æä¾›é«˜è´¨é‡åˆ†å—ä»¥æå‡åç»­æ£€ç´¢ä¸RAGæ•ˆæœ; It provides higher-quality chunks that improve later retrieval and RAG steps.
"""
Agentic Chunking Demo
Using LLM to intelligently split documents based on semantic meaning

This script demonstrates **Agentic Chunking** - the most advanced chunking method
where an AI model analyzes the document and decides optimal split points based on
topic shifts and semantic coherence, rather than arbitrary character counts.
"""

import os
import sys
from pathlib import Path

# å¯¼å…¥ï¼šLangChain ç»„ä»¶ä¸åˆ‡åˆ†å™¨ / Imports: LangChain components and splitters
# è¯´æ˜ï¼šæœ¬è„šæœ¬ä½¿ç”¨ LangChain çš„ ChatOpenAI è°ƒç”¨æ¨¡å‹ï¼Œå¹¶ç”¨æ–‡æœ¬åˆ‡åˆ†å™¨åšåŸºçº¿å¯¹æ¯” / This script uses LangChain ChatOpenAI and a splitter for baseline comparison.
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter


def load_dotenv(dotenv_path: Path, override: bool = False) -> None:
    """Minimal .env loader (no extra dependency).

    - Supports: KEY=VALUE
    - Ignores: blank lines and lines starting with '#'
    - override=True means .env wins over existing environment variables
    """

    # è¯»å– .envï¼šä¸å­˜åœ¨åˆ™ç›´æ¥è·³è¿‡ / Load .env: skip silently if missing
    if not dotenv_path.exists():
        return

    # é€è¡Œè§£æ KEY=VALUE / Parse KEY=VALUE lines
    for raw_line in dotenv_path.read_text(encoding="utf-8").splitlines():
        line = raw_line.strip()
        if not line or line.startswith("#"):
            continue
        if "=" not in line:
            continue
        key, value = line.split("=", 1)
        key = key.strip()
        value = value.strip().strip('"').strip("'")
        if not key:
            continue

        # å†™å…¥ç¯å¢ƒå˜é‡ï¼šoverride=True æ—¶ä»¥ .env ä¸ºå‡† / Write env vars: when override=True, .env wins
        if override or key not in os.environ:
            os.environ[key] = value


def sanitize_api_key(api_key: str) -> str:
    # æ¸…ç† keyï¼šå»æ‰é¦–å°¾ç©ºç™½ä¸å¯èƒ½çš„ Bearer å‰ç¼€ / Sanitize: trim whitespace and optional Bearer prefix
    api_key = (api_key or "").strip()
    if api_key.lower().startswith("bearer "):
        api_key = api_key[7:].strip()
    return api_key


# å¯åŠ¨æ¨ªå¹…ï¼šä¾¿äºåœ¨ç»ˆç«¯ä¸­åŒºåˆ†è„šæœ¬è¾“å‡º / Startup banner: easy to spot in terminal output
print("ğŸ¤– Agentic Chunking Demo")
print("=" * 50)

# è·¯å¾„å®šä½ï¼šä»¥å½“å‰è„šæœ¬ç›®å½•ä½œä¸ºé¡¹ç›®æ ¹ / Path: use script directory as project root
project_dir = Path(__file__).resolve().parent

# åŠ è½½é…ç½®ï¼šä¼˜å…ˆè¯»å–åŒç›®å½• .envï¼Œå¹¶è¦†ç›–åŒåç¯å¢ƒå˜é‡ / Config: load .env (same folder) and override existing vars
load_dotenv(project_dir / ".env", override=True)

# è¯»å– OpenAI ç›¸å…³é…ç½® / Read OpenAI-related settings
API_KEY = sanitize_api_key(os.environ.get("OPENAI_API_KEY", ""))
API_BASE = os.environ.get("OPENAI_API_BASE", "https://api.openai.com/v1").strip()
MODEL_NAME = os.environ.get("OPENAI_MODEL_NAME", "gpt-4.1-mini").strip()

# å…³é”®æ£€æŸ¥ï¼šæ²¡æœ‰ key å°±æ— æ³•è¿›è¡Œ Agentic Chunking / Guard: without API key, agentic chunking cannot run
if not API_KEY:
    print("âŒ Error: OPENAI_API_KEY not found.")
    print("Please create/update the .env file in this folder:")
    print(f"  {project_dir}\\.env")
    print("And set OPENAI_API_KEY=...")
    sys.exit(1)

# å®‰å…¨è¯Šæ–­ï¼šåªæ‰“å°é•¿åº¦/å°¾éƒ¨ï¼Œä¸æ³„éœ²å®Œæ•´ key / Safe diagnostics: never print full key
key_has_whitespace = any(ch.isspace() for ch in API_KEY)
key_tail = API_KEY[-4:] if len(API_KEY) >= 4 else "(short)"
print(f"ğŸ” API key loaded: length={len(API_KEY)}, tail={key_tail}, whitespace={key_has_whitespace}")

# è¾“å‡ºå½“å‰ç«¯ç‚¹ä¸æ¨¡å‹ï¼šä¾¿äºç¡®è®¤èµ°çš„æ˜¯å“ªä¸ªç¯å¢ƒé…ç½® / Show endpoint & model: confirm runtime configuration
print(f"ğŸ”Œ API Endpoint: {API_BASE}")
print(f"ğŸ§  Model: {MODEL_NAME}")
print()

# ç¤ºä¾‹æ–‡æ¡£ï¼šåŒ…å«å¤šä¸ªä¸»é¢˜æ®µè½ï¼Œç”¨äºå±•ç¤ºâ€œè¯­ä¹‰åˆ‡åˆ†â€çš„ä¼˜åŠ¿ / Sample doc: multiple topics to showcase semantic chunking
sample_document = """
TechCorp Company Overview

Company History: Founded in 1995 in a garage in Silicon Valley, TechCorp started as a small software consultancy. By 2000, it had grown to 500 employees and went public. The early years were marked by rapid expansion and the release of its flagship product, the TechOS. The founders, Jane Smith and John Doe, built the company on principles of innovation and customer focus.

Product Lineup: Today, TechCorp offers a wide range of enterprise software solutions. The CloudSuite is our most popular offering, providing scalable cloud infrastructure for businesses of all sizes. We also offer DataGuard for enterprise security, protecting sensitive data with military-grade encryption. AI-Core handles machine learning integration, making AI accessible to non-technical teams. Each product is designed to work seamlessly with the others.

Remote Work Policy: Employees may work remotely up to 3 days per week with manager approval. Remote work must be conducted using company-approved devices with VPN access enabled. All employees must be available during core hours (10 AM - 4 PM) and maintain regular communication with their team. Remote work is not a substitute for childcare or eldercare.

Future Vision: Looking ahead, TechCorp is betting big on quantum computing. We plan to invest $1B over the next 5 years in R&D for quantum technologies. Our goal is to be the first company to offer commercial quantum cloud services by 2030. This investment will create new positions for quantum researchers and engineers across all our locations.
"""

# æ–‡æ¡£æ¦‚è§ˆï¼šé•¿åº¦ä¸ä¸»é¢˜æ•°é‡ / Document overview: length and topic count
print("ğŸ“„ Sample Document:")
print(f"Length: {len(sample_document)} characters")
print("Contains 4 distinct topics: History, Products, Remote Work, Future")
print()

# å¯¹æ¯”å®éªŒï¼šå…ˆåšåŸºç¡€åˆ‡åˆ†ï¼Œå†åš agentic åˆ‡åˆ† / Comparison: basic chunking first, then agentic chunking
print("ğŸ”§ Comparison: Basic Chunking vs Agentic Chunking")
print("-" * 50)

# åŸºçº¿åˆ‡åˆ†å™¨ï¼šå­—ç¬¦é•¿åº¦ + åˆ†éš”ç¬¦ä¼˜å…ˆçº§ / Baseline splitter: character length + separator priority
basic_splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,
    chunk_overlap=50,
    separators=["\n\n", "\n", " ", ""],
)

# ç”ŸæˆåŸºç¡€ chunks å¹¶æ‰“å°é¢„è§ˆ / Build basic chunks and print previews
basic_chunks = basic_splitter.split_text(sample_document)
print(f"\nğŸ“Š Basic Chunking Result: {len(basic_chunks)} chunks")
print("   (Based on character count, may split mid-topic)")
for i, chunk in enumerate(basic_chunks, 1):
    preview = chunk[:60].replace("\n", " ").strip()
    print(f"   Chunk {i}: {preview}...")
print()


def agentic_chunking(text: str) -> list[str]:
    # Agentic Chunkingï¼šè®© LLM åˆ¤æ–­ä¸»é¢˜è¾¹ç•Œå¹¶è¾“å‡ºåˆ†éš”ç¬¦ / Agentic chunking: let the LLM decide topic boundaries
    print("ğŸ¤” Agent is analyzing the document for semantic topic shifts...")

    # åˆå§‹åŒ– ChatOpenAIï¼šä½¿ç”¨ç¯å¢ƒå˜é‡æä¾›çš„ key/base/model / Initialize ChatOpenAI with env-provided key/base/model
    llm = ChatOpenAI(
        model_name=MODEL_NAME,
        openai_api_key=API_KEY,
        openai_api_base=API_BASE,
        temperature=0,
    )

    # æ„é€ æç¤ºè¯ï¼šè¦æ±‚è¾“å‡ºä»¥ ---SPLIT--- åˆ†éš”çš„åŸæ–‡ chunks / Prompt: request chunks separated by ---SPLIT--- without rewriting content
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """You are an expert document editor specializing in semantic document analysis.
Your task is to split the provided text into semantically distinct chunks based on topic shifts.

Rules:
1. Keep related sentences together - don't break up a single topic
2. Split ONLY when the topic changes significantly (e.g., History -> Products -> Policy -> Future)
3. Each chunk should be about ONE coherent topic
4. Output the chunks separated by '---SPLIT---'
5. Do not modify the original text - just split it at appropriate boundaries
6. Include section headers with their content in the same chunk""",
            ),
            ("user", "{text}"),
        ]
    )

    # é“¾è·¯ï¼šPrompt -> LLM -> å­—ç¬¦ä¸²è§£æ / Chain: prompt -> LLM -> parse string output
    chain = prompt | llm | StrOutputParser()

    try:
        # è°ƒç”¨æ¨¡å‹å¹¶æŒ‰åˆ†éš”ç¬¦åˆ‡å— / Invoke model and split by delimiter
        response = chain.invoke({"text": text})
        return [c.strip() for c in response.split("---SPLIT---") if c.strip()]
    except Exception as e:
        # é”™è¯¯å¤„ç†ï¼šAPI å¤±è´¥æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œä¸è®©è„šæœ¬å´©æºƒ / Error handling: return [] on API failures
        print(f"\nâŒ API Error: {e}")
        return []


# æ‰§è¡Œ agentic chunkingï¼šå¾—åˆ°è¯­ä¹‰ chunks / Run agentic chunking: produce semantic chunks
agentic_chunks = agentic_chunking(sample_document)

# ç»“æœå±•ç¤ºï¼šæœ‰ç»“æœå°±é€å—æ‰“å°ä¸»é¢˜ä¸é¢„è§ˆ / Display: print topic and preview per chunk when available
if agentic_chunks:
    print(f"\nğŸ“Š Agentic Chunking Result: {len(agentic_chunks)} chunks")
    print("   (Based on semantic meaning and topic shifts)")
    print()

    # ç®€å•ä¸»é¢˜è¯†åˆ«ï¼šç”¨å…³é”®è¯çŒœæµ‹è¯¥ chunk çš„ä¸»é¢˜ / Lightweight topic labeling by keywords
    for i, chunk in enumerate(agentic_chunks, 1):
        if "History" in chunk or "Founded" in chunk:
            topic = "Company History"
        elif "Product" in chunk or "CloudSuite" in chunk:
            topic = "Products"
        elif "Remote" in chunk or "work" in chunk.lower():
            topic = "Remote Work Policy"
        elif "Future" in chunk or "quantum" in chunk.lower():
            topic = "Future Vision"
        else:
            topic = "General"

        print(f"ğŸ“¦ Chunk {i} - Topic: {topic}")
        print(f"   Length: {len(chunk)} characters")
        preview = chunk[:80].replace("\n", " ").strip()
        print(f"   Preview: {preview}...")
        print()

    # å¯¹æ¯”æ€»ç»“ï¼šåŸºç¡€åˆ‡åˆ† vs è¯­ä¹‰åˆ‡åˆ† / Summary: basic vs semantic
    print("ğŸ” Comparison Summary:")
    print("-" * 50)
    print(f"Basic Chunking:   {len(basic_chunks)} chunks (character-based)")
    print(f"Agentic Chunking: {len(agentic_chunks)} chunks (semantic-based)")
    print()
    print("ğŸ’¡ Key Differences:")
    print("âœ… Agentic chunking identifies natural topic boundaries")
    print("âœ… Each chunk contains ONE coherent topic")
    print("âœ… Better semantic coherence for RAG retrieval")
    print("âœ… AI understands context and meaning")
    print("âœ… No arbitrary character limit splitting")
    print()

    # å†™å…¥å®Œæˆæ ‡è®°ï¼šä¾¿äºè¯¾ç¨‹æ­¥éª¤æ£€æŸ¥ / Write completion marker for course workflow
    with open("agentic_chunking_complete.txt", "w", encoding="utf-8") as f:
        f.write("Agentic chunking demo completed successfully")

    print("âœ… Agentic chunking demo completed!")
else:
    # æ— ç»“æœï¼šé€šå¸¸æ˜¯ API é…ç½®/æƒé™/ç½‘ç»œé—®é¢˜ / No chunks: commonly API config/permission/network issue
    print("\nâš ï¸ Agent failed to produce chunks. Check API connection.")
