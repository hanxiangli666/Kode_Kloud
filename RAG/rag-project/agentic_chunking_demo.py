#!/usr/bin/env python3
"""
Agentic Chunking Demo
Using LLM to intelligently split documents based on semantic meaning

This script demonstrates **Agentic Chunking** - the most advanced chunking method
where an AI model analyzes the document and decides optimal split points based on
topic shifts and semantic coherence, rather than arbitrary character counts.
"""
import os
import sys
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ç¨‹åºå¯åŠ¨æç¤º / Startup banner
print("ğŸ¤– Agentic Chunking Demo")
print("=" * 50)

# é…ç½®ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡è¯»å– API è®¾ç½® / Config: read API settings from env vars
API_KEY = os.environ.get("OPENAI_API_KEY")
API_BASE = os.environ.get("OPENAI_API_BASE", "https://api.openai.com/v1")
MODEL_NAME = "openai/gpt-4.1-mini"

# å¦‚æœæ²¡æœ‰ API Key å°±ç›´æ¥é€€å‡º / Exit if API key is missing
if not API_KEY:
    print("âŒ Error: OPENAI_API_KEY not found.")
    print("Please ensure the environment is configured correctly.")
    sys.exit(1)

# æ‰“å°å½“å‰ä½¿ç”¨çš„ç«¯ç‚¹å’Œæ¨¡å‹ / Show selected endpoint and model
print(f"ğŸ”Œ API Endpoint: {API_BASE}")
print(f"ğŸ§  Model: {MODEL_NAME}")
print()

# ç¤ºä¾‹æ–‡æ¡£ï¼šåŒ…å«å¤šä¸ªä¸åŒä¸»é¢˜ / Sample document with multiple topics
sample_document = """
TechCorp Company Overview

Company History: Founded in 1995 in a garage in Silicon Valley, TechCorp started as a small software consultancy. By 2000, it had grown to 500 employees and went public. The early years were marked by rapid expansion and the release of its flagship product, the TechOS. The founders, Jane Smith and John Doe, built the company on principles of innovation and customer focus.

Product Lineup: Today, TechCorp offers a wide range of enterprise software solutions. The CloudSuite is our most popular offering, providing scalable cloud infrastructure for businesses of all sizes. We also offer DataGuard for enterprise security, protecting sensitive data with military-grade encryption. AI-Core handles machine learning integration, making AI accessible to non-technical teams. Each product is designed to work seamlessly with the others.

Remote Work Policy: Employees may work remotely up to 3 days per week with manager approval. Remote work must be conducted using company-approved devices with VPN access enabled. All employees must be available during core hours (10 AM - 4 PM) and maintain regular communication with their team. Remote work is not a substitute for childcare or eldercare.

Future Vision: Looking ahead, TechCorp is betting big on quantum computing. We plan to invest $1B over the next 5 years in R&D for quantum technologies. Our goal is to be the first company to offer commercial quantum cloud services by 2030. This investment will create new positions for quantum researchers and engineers across all our locations.
"""

# æ˜¾ç¤ºç¤ºä¾‹æ–‡æ¡£çš„åŸºæœ¬ä¿¡æ¯ / Show basic info for the sample doc
print("ğŸ“„ Sample Document:")
print(f"Length: {len(sample_document)} characters")
print(f"Contains 4 distinct topics: History, Products, Remote Work, Future")
print()

# å…ˆä¸åŸºç¡€åˆ‡åˆ†æ–¹å¼åšå¯¹æ¯” / Compare with basic chunking first
print("ğŸ”§ Comparison: Basic Chunking vs Agentic Chunking")
print("-" * 50)

# åŸºç¡€åˆ‡åˆ†ï¼šæŒ‰å­—ç¬¦æ•°è¿›è¡Œåˆ‡å— / Basic chunking by character count
basic_splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,
    chunk_overlap=50,
    separators=["\n\n", "\n", " ", ""]
)

# ç”ŸæˆåŸºç¡€åˆ‡åˆ†ç»“æœå¹¶é¢„è§ˆ / Build basic chunks and preview
basic_chunks = basic_splitter.split_text(sample_document)
print(f"\nğŸ“Š Basic Chunking Result: {len(basic_chunks)} chunks")
print("   (Based on character count, may split mid-topic)")
for i, chunk in enumerate(basic_chunks, 1):
    preview = chunk[:60].replace('\n', ' ').strip()
    print(f"   Chunk {i}: {preview}...")
print()

# ä½¿ç”¨ LLM è¿›è¡Œ Agentic Chunking / Agentic chunking with an LLM
def agentic_chunking(text):
    """
    Uses an LLM to split text into semantically distinct chunks.
    The AI analyzes topic shifts and creates meaningful boundaries.
    """
    # æç¤ºæ­£åœ¨è¿›è¡Œè¯­ä¹‰åˆ†æ / Indicate semantic analysis is running
    print("ğŸ¤” Agent is analyzing the document for semantic topic shifts...")
    
    # åˆå§‹åŒ– LLM å®¢æˆ·ç«¯ / Initialize LLM client
    llm = ChatOpenAI(
        model=MODEL_NAME,
        openai_api_key=API_KEY,
        openai_api_base=API_BASE,
        temperature=0  # ç¡®ä¿è¾“å‡ºç¨³å®š / Deterministic output for consistency
    )

    # æ„é€ æç¤ºè¯ï¼šè®©æ¨¡å‹å……å½“â€œåˆ‡åˆ†ä»£ç†â€ / Prompt: instruct the model to chunk
    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are an expert document editor specializing in semantic document analysis.
Your task is to split the provided text into semantically distinct chunks based on topic shifts.

Rules:
1. Keep related sentences together - don't break up a single topic
2. Split ONLY when the topic changes significantly (e.g., History -> Products -> Policy -> Future)
3. Each chunk should be about ONE coherent topic
4. Output the chunks separated by '---SPLIT---'
5. Do not modify the original text - just split it at appropriate boundaries
6. Include section headers with their content in the same chunk"""),
        ("user", "{text}")
    ])

    # ç»„åˆé“¾è·¯ï¼šæç¤ºè¯ -> æ¨¡å‹ -> æ–‡æœ¬è¾“å‡ºè§£æ / Chain: prompt -> model -> parser
    chain = prompt | llm | StrOutputParser()
    
    try:
        # è°ƒç”¨æ¨¡å‹å¹¶æŒ‰åˆ†éš”ç¬¦åˆ‡åˆ†è¾“å‡º / Invoke and split by delimiter
        response = chain.invoke({"text": text})
        # æŒ‰åˆ†éš”ç¬¦åˆ‡åˆ†å¹¶æ¸…ç† / Split by delimiter and clean up
        chunks = [c.strip() for c in response.split("---SPLIT---") if c.strip()]
        return chunks
    except Exception as e:
        # æ•è· API é”™è¯¯å¹¶è¿”å›ç©ºç»“æœ / Handle API errors and return empty result
        print(f"\nâŒ API Error: {e}")
        return []

# æ‰§è¡Œ Agentic Chunking / Run agentic chunking
agentic_chunks = agentic_chunking(sample_document)

# æ ¹æ®æ˜¯å¦æœ‰ç»“æœè¾“å‡ºä¸åŒå†…å®¹ / Branch based on result presence
if agentic_chunks:
    print(f"\nğŸ“Š Agentic Chunking Result: {len(agentic_chunks)} chunks")
    print("   (Based on semantic meaning and topic shifts)")
    print()
    
    # é€å—åˆ¤æ–­ä¸»é¢˜å¹¶æ‰“å°é¢„è§ˆ / Detect topic and print preview per chunk
    for i, chunk in enumerate(agentic_chunks, 1):
        # ä»å†…å®¹åˆ¤æ–­ä¸»é¢˜ / Identify the likely topic from the chunk
        if "History" in chunk or "Founded" in chunk:
            topic = "Company History"
        elif "Product" in chunk or "CloudSuite" in chunk:
            topic = "Products"
        elif "Remote" in chunk or "work" in chunk.lower():
            topic = "Remote Work Policy"
        elif "Future" in chunk or "quantum" in chunk.lower():
            topic = "Future Vision"
        else:
            topic = "General"
        
        print(f"ğŸ“¦ Chunk {i} - Topic: {topic}")
        print(f"   Length: {len(chunk)} characters")
        preview = chunk[:80].replace('\n', ' ').strip()
        print(f"   Preview: {preview}...")
        print()

    # å¯¹æ¯”æ€»ç»“ / Comparison summary
    print("ğŸ” Comparison Summary:")
    print("-" * 50)
    print(f"Basic Chunking:   {len(basic_chunks)} chunks (character-based)")
    print(f"Agentic Chunking: {len(agentic_chunks)} chunks (semantic-based)")
    print()
    print("ğŸ’¡ Key Differences:")
    print("âœ… Agentic chunking identifies natural topic boundaries")
    print("âœ… Each chunk contains ONE coherent topic")
    print("âœ… Better semantic coherence for RAG retrieval")
    print("âœ… AI understands context and meaning")
    print("âœ… No arbitrary character limit splitting")
    print()
    
    print("ğŸ’¡ When to Use Agentic Chunking:")
    print("âœ… Documents with clear topic sections")
    print("âœ… When semantic coherence is critical")
    print("âœ… Complex documents with mixed content")
    print("âœ… When retrieval quality matters more than speed")
    print()
    
    print("âš ï¸  Considerations:")
    print("â€¢ Requires LLM API calls (cost and latency)")
    print("â€¢ Best for smaller documents or preprocessing")
    print("â€¢ May need fallback for very large documents")
    
    # å†™å…¥å®Œæˆæ ‡è®°æ–‡ä»¶ / Write completion marker file
    with open("agentic_chunking_complete.txt", "w") as f:
        f.write("Agentic chunking demo completed successfully")
    
    print("\nâœ… Agentic chunking demo completed!")
else:
    # æ— ç»“æœæ—¶æç¤ºæ£€æŸ¥ API / Warn when no chunks returned
    print("\nâš ï¸ Agent failed to produce chunks. Check API connection.")
